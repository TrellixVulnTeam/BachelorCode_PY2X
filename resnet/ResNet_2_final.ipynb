{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# ResNet kvantiz√°cia\n",
    "\n",
    "## Obsah\n",
    "\n",
    "*   [Predspracovanie](#predspracovanie)\n",
    "<a href='#predspracovanie'> </a>\n",
    "*   [Statistiky na staticku kvant.](#stats)\n",
    "<a href='#stats'> </a>\n",
    "*   [Base precision](#base)\n",
    "<a href='#base'> </a>\n",
    "*   Range-Based Quantization\n",
    "    *   [8 bit kvantizacia](#8bit)\n",
    "<a href='#8bit'> </a>\n",
    "    *   [4 bit kvantizacia](#4bit)\n",
    "<a href='#4bit'> </a>\n",
    "*   Loss-Aware Quantization\n",
    "    *   [8 bit kvantizacia](#8bit-loss)\n",
    "<a href='#8bit-loss'> </a>\n",
    "    *   [4 bit kvantizacia](#4bit-loss)\n",
    "<a href='#4bit-loss'> </a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.1\n",
      "0.4.2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import distiller\n",
    "from distiller.models import create_model\n",
    "\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id='predspracovanie'> </a>\n",
    "\n",
    "# Predspracovanie"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "model = create_model(pretrained=True,dataset='imagenet',arch='resnet18') "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "DataParallel(\n  (module): ResNet(\n    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu): ReLU(inplace=True)\n    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (layer1): Sequential(\n      (0): DistillerBasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (add): EltwiseAdd()\n        (relu2): ReLU(inplace=True)\n      )\n      (1): DistillerBasicBlock(\n        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (add): EltwiseAdd()\n        (relu2): ReLU(inplace=True)\n      )\n    )\n    (layer2): Sequential(\n      (0): DistillerBasicBlock(\n        (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (add): EltwiseAdd()\n        (relu2): ReLU(inplace=True)\n      )\n      (1): DistillerBasicBlock(\n        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (add): EltwiseAdd()\n        (relu2): ReLU(inplace=True)\n      )\n    )\n    (layer3): Sequential(\n      (0): DistillerBasicBlock(\n        (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (add): EltwiseAdd()\n        (relu2): ReLU(inplace=True)\n      )\n      (1): DistillerBasicBlock(\n        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (add): EltwiseAdd()\n        (relu2): ReLU(inplace=True)\n      )\n    )\n    (layer4): Sequential(\n      (0): DistillerBasicBlock(\n        (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (add): EltwiseAdd()\n        (relu2): ReLU(inplace=True)\n      )\n      (1): DistillerBasicBlock(\n        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (add): EltwiseAdd()\n        (relu2): ReLU(inplace=True)\n      )\n    )\n    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n    (fc): Linear(in_features=512, out_features=1000, bias=True)\n  )\n)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "preprocessing = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "batch_size = 40\n",
    "num_workers = 1\n",
    "dataset = torchvision.datasets.ImageFolder('/home/bohumil/FIIT/BP/BP/Zdroje_kod/imagenet/val'\n",
    "                                           ,preprocessing)\n",
    "\n",
    "small, big = torch.utils.data.random_split(dataset,[7000, len(dataset)-7000])\n",
    " \n",
    "dataloader = torch.utils.data.DataLoader(small,batch_size=batch_size,\n",
    "                                         num_workers=num_workers,shuffle=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from resnet_output import resnet_output\n",
    "\n",
    "def target_labels(dataset,target):\n",
    "    list = target.tolist()\n",
    "    for i in range(len(list)):\n",
    "        list[i] = dataset.classes[list[i]]\n",
    "        list[i] = resnet_output[list[i]]\n",
    "    return torch.LongTensor(list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# z <distiller_root>/jupyter/post_train_quant_convert_pytorch.ipynb\n",
    "import torchnet as tnt\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def eval_model(data_loader, model, device='cpu', print_freq=10):\n",
    "    # print('Evaluation model ', model.arch)\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "    \n",
    "    loss = tnt.meter.AverageValueMeter()\n",
    "    classerr = tnt.meter.ClassErrorMeter(accuracy=True, topk=(1, 5))\n",
    "    # apmeter = tnt.meter.APMeter()\n",
    "\n",
    "    total_samples = len(data_loader.sampler)\n",
    "    batch_size = data_loader.batch_size\n",
    "    total_steps = math.ceil(total_samples / batch_size)\n",
    "    print('{0} samples ({1} per mini-batch)'.format(total_samples, batch_size))\n",
    "\n",
    "    # Switch to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    for step, (inputs, target) in enumerate(data_loader):\n",
    "        with torch.no_grad():\n",
    "            inputs, target = inputs.to(device), target.to(device)\n",
    "            # compute output from model\n",
    "            output = model(inputs)\n",
    "            target = target_labels(dataset,target).to(device)\n",
    "            # compute loss and measure accuracy\n",
    "            loss.add(criterion(output, target).item())\n",
    "            classerr.add(output.data, target)\n",
    "\n",
    "            if (step + 1) % print_freq == 0:\n",
    "                print('[{:3d}/{:3d}] Top1: {:.3f}  Top5: {:.3f}  Loss: {:.3f}'.format(\n",
    "                      step + 1, total_steps, classerr.value(1), classerr.value(5), loss.mean), flush=True)\n",
    "    print('----------')\n",
    "    print('Overall ==> Top1: {:.3f}  Top5: {:.3f}  Loss: {:.3f}  PPL: {:.3f}'.format(\n",
    "        classerr.value(1), classerr.value(5), loss.mean, np.exp(loss.mean)), flush=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import logging\n",
    "def config_notebooks_logger():\n",
    "    logging.config.fileConfig('logging.conf')\n",
    "    msglogger = logging.getLogger()\n",
    "    msglogger.info('Logging configured successfully')\n",
    "    return msglogger"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Logging configured successfully\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import distiller\n",
    "\n",
    "msglogger = config_notebooks_logger()\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "distiller.quantization.add_post_train_quant_args(parser)\n",
    "args = parser.parse_args(args= [])\n",
    "# args.qe_config_file = '/home/bohumil/FIIT/BP/BP/Zdroje_kod/quantization_jupyters/resnet18_imagenet_post_train.yaml'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id='stats'> </a>\n",
    "\n",
    "# Correct way of getting statistics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cpu_model = distiller.make_non_parallel_copy(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from distiller.data_loggers import collect_quant_stats, QuantCalibrationStatsCollector, collector_context\n",
    "\n",
    "\n",
    "args.qe_calibration = 0.2\n",
    "if args.qe_calibration:\n",
    "    \n",
    "    cpu_model = distiller.make_non_parallel_copy(model).cpu()\n",
    "    \n",
    "    distiller.utils.assign_layer_fq_names(cpu_model)\n",
    "    msglogger.info(\"Generating quantization calibration stats based on {0} users\".format(args.qe_calibration))\n",
    "    collector = distiller.data_loggers.QuantCalibrationStatsCollector(cpu_model)\n",
    "    with collector_context(collector):\n",
    "        eval_model(train_loader_gpu,cpu_model,'cuda',print_freq=30)\n",
    "        # Here call your model evaluation function, making sure to execute only\n",
    "        # the portion of the dataset specified by the qe_calibration argument\n",
    "    yaml_path = './act_quantization_stats.yaml'\n",
    "    collector.save(yaml_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a href='#base'> </a>\n",
    "\n",
    "# Base precision"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000 samples (40 per mini-batch)\n",
      "[ 40/175] Top1: 76.250  Top5: 93.688  Loss: 0.885\n",
      "[ 80/175] Top1: 78.156  Top5: 93.719  Loss: 0.845\n",
      "[120/175] Top1: 78.208  Top5: 93.896  Loss: 0.838\n",
      "[160/175] Top1: 78.328  Top5: 93.922  Loss: 0.833\n",
      "----------\n",
      "Overall ==> Top1: 78.286  Top5: 93.943  Loss: 0.832  PPL: 2.298\n",
      "CPU times: user 1min 29s, sys: 841 ms, total: 1min 29s\n",
      "Wall time: 1min 31s\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    %time eval_model(dataloader,model,'cuda', print_freq=40)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "args.quantize_eval = True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "def eval_quantized(model, args):\n",
    "    if args.quantize_eval:\n",
    "        quantizer = distiller.quantization.PostTrainLinearQuantizer.from_args(deepcopy(model), args)\n",
    "        # dummy = distiller.get_dummy_input(model.input_shape)\n",
    "        dummy = distiller.get_dummy_input(input_shape=model.input_shape)\n",
    "        quantizer.prepare_model(dummy)\n",
    "        eval_model(dataloader, quantizer.model, 'cuda', print_freq=30)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id='8bit'> </a>\n",
    "\n",
    "# 8 bit quantization"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading configuration from: /home/bohumil/FIIT/BP/BP/Zdroje_kod/quantization_jupyters/resnet/resnet18/resnet18_imagenet_post_train.yaml\n",
      "Found component of class PostTrainLinearQuantizer: Name: post_train_quantizer ; Section: quantizers\n",
      "Loading activation stats from: /home/bohumil/FIIT/BP/BP/Zdroje_kod/quantization_jupyters/resnet/acts_quantization_stats.yaml\n",
      "Preparing model for quantization using PostTrainLinearQuantizer\n",
      "Applying batch-norm folding ahead of post-training quantization\n",
      "Propagating output statistics from BN modules to folded modules\n",
      "Optimizing output statistics for modules followed by ReLU/Tanh/Sigmoid\n",
      "Updated stats saved to ./quant_stats_after_prepare_model.yaml\n",
      "Per-layer quantization parameters saved to ./layer_quant_params.yaml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000 samples (40 per mini-batch)\n",
      "[ 30/175] Top1: 78.583  Top5: 93.750  Loss: 0.813\n",
      "[ 60/175] Top1: 78.500  Top5: 94.083  Loss: 0.822\n",
      "[ 90/175] Top1: 78.389  Top5: 93.944  Loss: 0.832\n",
      "[120/175] Top1: 77.875  Top5: 93.854  Loss: 0.842\n",
      "[150/175] Top1: 78.250  Top5: 93.850  Loss: 0.834\n",
      "----------\n",
      "Overall ==> Top1: 78.214  Top5: 93.886  Loss: 0.837  PPL: 2.310\n"
     ]
    }
   ],
   "source": [
    "args.qe_config_file = '/home/bohumil/FIIT/BP/BP/Zdroje_kod/quantization_jupyters/resnet/resnet18/resnet18_imagenet_post_train.yaml'\n",
    "eval_quantized(model, args)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id='4bit'> </a>\n",
    "\n",
    "# 4 bit quantization\n",
    "\n",
    "## Run 1\n",
    "```python\n",
    "class: PostTrainLinearQuantizer\n",
    "bits_activations: 4\n",
    "bits_parameters: 4\n",
    "bits_accum: 16\n",
    "mode: ASYMMETRIC_UNSIGNED\n",
    "per_channel_wts: True\n",
    "clip_acts: AVG\n",
    "\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading configuration from: /home/bohumil/FIIT/BP/BP/Zdroje_kod/quantization_jupyters/resnet/resnet18/resnet18_imagenet_post_train_4bit.yaml\n",
      "Found component of class PostTrainLinearQuantizer: Name: post_train_quantizer ; Section: quantizers\n",
      "Loading activation stats from: /home/bohumil/FIIT/BP/BP/Zdroje_kod/quantization_jupyters/resnet/acts_quantization_stats.yaml\n",
      "Preparing model for quantization using PostTrainLinearQuantizer\n",
      "Applying batch-norm folding ahead of post-training quantization\n",
      "Propagating output statistics from BN modules to folded modules\n",
      "Optimizing output statistics for modules followed by ReLU/Tanh/Sigmoid\n",
      "Updated stats saved to ./quant_stats_after_prepare_model.yaml\n",
      "Per-layer quantization parameters saved to ./layer_quant_params.yaml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000 samples (40 per mini-batch)\n",
      "[ 30/175] Top1: 28.167  Top5: 51.000  Loss: 3.792\n",
      "[ 60/175] Top1: 28.000  Top5: 50.792  Loss: 3.782\n",
      "[ 90/175] Top1: 26.306  Top5: 50.389  Loss: 3.864\n",
      "[120/175] Top1: 26.812  Top5: 51.083  Loss: 3.829\n",
      "[150/175] Top1: 26.833  Top5: 50.600  Loss: 3.832\n",
      "----------\n",
      "Overall ==> Top1: 27.157  Top5: 50.829  Loss: 3.805  PPL: 44.947\n"
     ]
    }
   ],
   "source": [
    "args.qe_config_file = '/home/bohumil/FIIT/BP/BP/Zdroje_kod/quantization_jupyters/resnet/resnet18/resnet18_imagenet_post_train_4bit.yaml'\n",
    "eval_quantized(model, args)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Uprava parametrov\n",
    "## Run 2\n",
    "/home/bohumil/FIIT/BP/BP/Zdroje_kod/quantization_jupyters/resnet18_imagenet_post_train_4bit2.yaml\n",
    "\n",
    "```python\n",
    "quantizers:\n",
    "  post_train_quantizer:\n",
    "    class: PostTrainLinearQuantizer\n",
    "    bits_activations: 4\n",
    "    bits_parameters: 4\n",
    "    bits_accum: 16\n",
    "\n",
    "    mode: ASYMMETRIC_UNSIGNED\n",
    "    \n",
    "    model_activation_stats: /home/bohumil/FIIT/BP/BP/Zdroje_kod/quantization_jupyters/acts_quantization_stats.yaml\n",
    "    per_channel_wts: True\n",
    "    clip_acts: AVG\n",
    "\n",
    "    overrides:\n",
    "      fc:\n",
    "        clip_acts: NONE  # Don't clip activations in last layer before softmax\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading configuration from: /home/bohumil/FIIT/BP/BP/Zdroje_kod/quantization_jupyters/resnet/resnet18/resnet18_imagenet_post_train_4bit2.yaml\n",
      "Found component of class PostTrainLinearQuantizer: Name: post_train_quantizer ; Section: quantizers\n",
      "Loading activation stats from: /home/bohumil/FIIT/BP/BP/Zdroje_kod/quantization_jupyters/resnet/acts_quantization_stats.yaml\n",
      "Preparing model for quantization using PostTrainLinearQuantizer\n",
      "Applying batch-norm folding ahead of post-training quantization\n",
      "Propagating output statistics from BN modules to folded modules\n",
      "Optimizing output statistics for modules followed by ReLU/Tanh/Sigmoid\n",
      "Updated stats saved to ./quant_stats_after_prepare_model.yaml\n",
      "Per-layer quantization parameters saved to ./layer_quant_params.yaml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000 samples (40 per mini-batch)\n",
      "[ 30/175] Top1: 21.500  Top5: 43.167  Loss: 4.409\n",
      "[ 60/175] Top1: 22.917  Top5: 44.375  Loss: 4.271\n",
      "[ 90/175] Top1: 22.917  Top5: 44.444  Loss: 4.244\n",
      "[120/175] Top1: 22.896  Top5: 43.937  Loss: 4.247\n",
      "[150/175] Top1: 22.717  Top5: 44.133  Loss: 4.248\n",
      "----------\n",
      "Overall ==> Top1: 22.786  Top5: 44.029  Loss: 4.263  PPL: 71.023\n"
     ]
    }
   ],
   "source": [
    "args.qe_config_file = '/home/bohumil/FIIT/BP/BP/Zdroje_kod/quantization_jupyters/resnet/resnet18/resnet18_imagenet_post_train_4bit2.yaml'\n",
    "eval_quantized(model, args)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "```python\n",
    " overrides:\n",
    "    # First and last layers in 8-bits\n",
    "      conv1:\n",
    "        bits_weights: 8\n",
    "        bits_activations: 8\n",
    "      fc:\n",
    "        bits_weights: 8\n",
    "        bits_activations: 8\n",
    "        clip_acts: NONE  # Don't clip activations in last layer before softmax\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading configuration from: /home/bohumil/FIIT/BP/BP/Zdroje_kod/quantization_jupyters/resnet/resnet18/resnet18_imagenet_post_train_4bit3.yaml\n",
      "Found component of class PostTrainLinearQuantizer: Name: post_train_quantizer ; Section: quantizers\n",
      "Loading activation stats from: /home/bohumil/FIIT/BP/BP/Zdroje_kod/quantization_jupyters/resnet/acts_quantization_stats.yaml\n",
      "Preparing model for quantization using PostTrainLinearQuantizer\n",
      "Applying batch-norm folding ahead of post-training quantization\n",
      "Propagating output statistics from BN modules to folded modules\n",
      "Optimizing output statistics for modules followed by ReLU/Tanh/Sigmoid\n",
      "Updated stats saved to ./quant_stats_after_prepare_model.yaml\n",
      "Per-layer quantization parameters saved to ./layer_quant_params.yaml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000 samples (40 per mini-batch)\n",
      "[ 30/175] Top1: 0.167  Top5: 0.833  Loss: 9.153\n",
      "[ 60/175] Top1: 0.208  Top5: 1.167  Loss: 9.105\n",
      "[ 90/175] Top1: 0.139  Top5: 1.000  Loss: 9.076\n",
      "[120/175] Top1: 0.146  Top5: 1.021  Loss: 9.098\n",
      "[150/175] Top1: 0.133  Top5: 0.900  Loss: 9.102\n",
      "----------\n",
      "Overall ==> Top1: 0.143  Top5: 0.871  Loss: 9.095  PPL: 8908.387\n"
     ]
    }
   ],
   "source": [
    "args.qe_config_file = '/home/bohumil/FIIT/BP/BP/Zdroje_kod/quantization_jupyters/resnet/resnet18/resnet18_imagenet_post_train_4bit3.yaml'\n",
    "eval_quantized(model, args)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Mixed model\n",
    "\n",
    "```python\n",
    "quantizers:\n",
    "  post_train_quantizer:\n",
    "    class: PostTrainLinearQuantizer\n",
    "    bits_activations: 8\n",
    "    bits_parameters: 4\n",
    "    bits_accum: 16\n",
    "\n",
    "    mode: ASYMMETRIC_UNSIGNED\n",
    "   \n",
    "    model_activation_stats: /home/bohumil/FIIT/BP/BP/Zdroje_kod/quantization_jupyters/acts_quantization_stats.yaml\n",
    "    per_channel_wts: True\n",
    "    clip_acts: AVG\n",
    "\n",
    "    overrides:\n",
    "    # First and last layers in 8-bits\n",
    "      conv1:\n",
    "        bits_weights: 8\n",
    "        bits_activations: 8\n",
    "      fc:\n",
    "        bits_weights: 8\n",
    "        bits_activations: 8\n",
    "        clip_acts: NONE  # Don't clip activations in last layer before softmax\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading configuration from: /home/bohumil/FIIT/BP/BP/Zdroje_kod/quantization_jupyters/resnet/resnet18/resnet18_imagenet_post_train_4bit4.yaml\n",
      "Found component of class PostTrainLinearQuantizer: Name: post_train_quantizer ; Section: quantizers\n",
      "Loading activation stats from: /home/bohumil/FIIT/BP/BP/Zdroje_kod/quantization_jupyters/resnet/acts_quantization_stats.yaml\n",
      "Preparing model for quantization using PostTrainLinearQuantizer\n",
      "Applying batch-norm folding ahead of post-training quantization\n",
      "Propagating output statistics from BN modules to folded modules\n",
      "Optimizing output statistics for modules followed by ReLU/Tanh/Sigmoid\n",
      "Updated stats saved to ./quant_stats_after_prepare_model.yaml\n",
      "Per-layer quantization parameters saved to ./layer_quant_params.yaml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000 samples (40 per mini-batch)\n",
      "[ 30/175] Top1: 0.000  Top5: 1.000  Loss: 9.346\n",
      "[ 60/175] Top1: 0.083  Top5: 1.125  Loss: 9.283\n",
      "[ 90/175] Top1: 0.083  Top5: 1.028  Loss: 9.266\n",
      "[120/175] Top1: 0.125  Top5: 1.104  Loss: 9.300\n",
      "[150/175] Top1: 0.133  Top5: 1.117  Loss: 9.333\n",
      "----------\n",
      "Overall ==> Top1: 0.157  Top5: 1.171  Loss: 9.323  PPL: 11187.213\n"
     ]
    }
   ],
   "source": [
    "args.qe_config_file = '/home/bohumil/FIIT/BP/BP/Zdroje_kod/quantization_jupyters/resnet/resnet18/resnet18_imagenet_post_train_4bit4.yaml'\n",
    "eval_quantized(model, args)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id='8bit-loss'> </a>\n",
    "\n",
    "# 8 bit Loss-Aware quantization\n",
    "\n",
    "Distiller supports Loss-Aware quantization in its sample application."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log file for this run: /home/bohumil/FIIT/BP/BP/Zdroje_kod/quantization_jupyters/resnet/logs/2020.04.23-135409/2020.04.23-135409.log\r\n",
      "Random seed: 0\r\n",
      "\r\n",
      "--------------------------------------------------------\r\n",
      "Logging to TensorBoard - remember to execute the server:\r\n",
      "> tensorboard --logdir='./logs'\r\n",
      "\r\n",
      "=> created a pretrained resnet18 model with the imagenet dataset\r\n",
      "Dataset sizes:\r\n",
      "\ttest=19439\r\n",
      "Dataset sizes:\r\n",
      "\ttest=19439\r\n",
      "Reading configuration from: /home/bohumil/FIIT/BP/BP/Zdroje_kod/distiller/examples/quantization/post_train_quant/resnet18_imagenet_post_train_lapq.yaml\r\n",
      "Found component of class PostTrainLinearQuantizer: Name: post_train_quantizer ; Section: quantizers\r\n",
      "Loading activation stats from: /home/bohumil/FIIT/BP/BP/Zdroje_kod/distiller/examples/quantization/post_train_quant/stats/resnet18_quant_stats.yaml\r\n",
      "Initializing quantizer...\r\n",
      "Preparing model for quantization using PostTrainLinearQuantizer\r\n",
      "Applying batch-norm folding ahead of post-training quantization\r\n",
      "Propagating output statistics from BN modules to folded modules\r\n",
      "Optimizing output statistics for modules followed by ReLU/Tanh/Sigmoid\r\n",
      "Updated stats saved to logs/2020.04.23-135409/quant_stats_after_prepare_model.yaml\r\n",
      "Per-layer quantization parameters saved to logs/2020.04.23-135409/layer_quant_params.yaml\r\n",
      "Initializing quantization parameters...\r\n",
      "module.layer1.0.conv1\r\n",
      "module.layer1.0.relu1\r\n",
      "module.layer1.0.conv2\r\n",
      "module.layer1.0.relu2\r\n",
      "module.layer1.1.conv1\r\n",
      "module.layer1.1.relu1\r\n",
      "module.layer1.1.conv2\r\n",
      "module.layer1.1.relu2\r\n",
      "module.layer2.0.conv1\r\n",
      "module.layer2.0.downsample.0\r\n",
      "module.layer2.0.relu1\r\n",
      "module.layer2.0.conv2\r\n",
      "module.layer2.0.relu2\r\n",
      "module.layer2.1.conv1\r\n",
      "module.layer2.1.relu1\r\n",
      "module.layer2.1.conv2\r\n",
      "module.layer2.1.relu2\r\n",
      "module.layer3.0.conv1\r\n",
      "module.layer3.0.downsample.0\r\n",
      "module.layer3.0.relu1\r\n",
      "module.layer3.0.conv2\r\n",
      "module.layer3.0.relu2\r\n",
      "module.layer3.1.conv1\r\n",
      "module.layer3.1.relu1\r\n",
      "module.layer3.1.conv2\r\n",
      "module.layer3.1.relu2\r\n",
      "module.layer4.0.conv1\r\n",
      "module.layer4.0.downsample.0\r\n",
      "module.layer4.0.relu1\r\n",
      "module.layer4.0.conv2\r\n",
      "module.layer4.0.relu2\r\n",
      "module.layer4.1.conv1\r\n",
      "module.layer4.1.relu1\r\n",
      "Evaluating initial quantization score...\r\n",
      "--- test ---------------------\r\n",
      "19439 samples (20 per mini-batch)\r\n",
      "Test: [   30/  971]    Loss 11.953374    Top1 0.000000    Top5 0.000000    \r\n",
      "Test: [   60/  971]    Loss 12.026128    Top1 0.000000    Top5 0.083333    \r\n",
      "Test: [   90/  971]    Loss 12.125166    Top1 0.000000    Top5 0.055556    \r\n",
      "Test: [  120/  971]    Loss 12.086123    Top1 0.041667    Top5 0.125000    \r\n",
      "Test: [  150/  971]    Loss 12.079851    Top1 0.033333    Top5 0.133333    \r\n",
      "Test: [  180/  971]    Loss 12.089974    Top1 0.027778    Top5 0.166667    \r\n",
      "Test: [  210/  971]    Loss 12.072640    Top1 0.023810    Top5 0.214286    \r\n",
      "Test: [  240/  971]    Loss 12.073649    Top1 0.020833    Top5 0.229167    \r\n",
      "Test: [  270/  971]    Loss 12.045526    Top1 0.018519    Top5 0.240741    \r\n",
      "Test: [  300/  971]    Loss 12.064353    Top1 0.016667    Top5 0.266667    \r\n",
      "Test: [  330/  971]    Loss 12.067106    Top1 0.030303    Top5 0.272727    \r\n",
      "Test: [  360/  971]    Loss 12.070958    Top1 0.027778    Top5 0.250000    \r\n",
      "Test: [  390/  971]    Loss 12.057973    Top1 0.025641    Top5 0.294872    \r\n",
      "Test: [  420/  971]    Loss 12.058486    Top1 0.023810    Top5 0.333333    \r\n",
      "Test: [  450/  971]    Loss 12.048095    Top1 0.022222    Top5 0.311111    \r\n",
      "Test: [  480/  971]    Loss 12.043112    Top1 0.020833    Top5 0.322917    \r\n",
      "Test: [  510/  971]    Loss 12.024861    Top1 0.019608    Top5 0.333333    \r\n",
      "Test: [  540/  971]    Loss 12.022269    Top1 0.018519    Top5 0.314815    \r\n",
      "Test: [  570/  971]    Loss 12.029420    Top1 0.017544    Top5 0.333333    \r\n",
      "Test: [  600/  971]    Loss 12.028556    Top1 0.016667    Top5 0.333333    \r\n",
      "Test: [  630/  971]    Loss 12.032801    Top1 0.015873    Top5 0.325397    \r\n",
      "Test: [  660/  971]    Loss 12.036199    Top1 0.015152    Top5 0.310606    \r\n",
      "Test: [  690/  971]    Loss 12.031686    Top1 0.014493    Top5 0.318841    \r\n",
      "Test: [  720/  971]    Loss 12.033525    Top1 0.020833    Top5 0.326389    \r\n",
      "Test: [  750/  971]    Loss 12.038199    Top1 0.020000    Top5 0.320000    \r\n",
      "Test: [  780/  971]    Loss 12.041913    Top1 0.019231    Top5 0.314103    \r\n",
      "Test: [  810/  971]    Loss 12.038865    Top1 0.018519    Top5 0.314815    \r\n",
      "Test: [  840/  971]    Loss 12.037861    Top1 0.017857    Top5 0.309524    \r\n",
      "Test: [  870/  971]    Loss 12.033100    Top1 0.017241    Top5 0.316092    \r\n",
      "Test: [  900/  971]    Loss 12.026800    Top1 0.016667    Top5 0.316667    \r\n",
      "Test: [  930/  971]    Loss 12.023191    Top1 0.016129    Top5 0.322581    \r\n",
      "Test: [  960/  971]    Loss 12.021982    Top1 0.015625    Top5 0.328125    \r\n",
      "==> Top1: 0.015    Top5: 0.334    Loss: 12.023\r\n",
      "\r\n",
      "Evaluation set loss after initialization 12.023\r\n",
      "Evaluating on full test set...\r\n",
      "--- test ---------------------\r\n",
      "19439 samples (20 per mini-batch)\r\n",
      "Test: [   30/  971]    Loss 11.861073    Top1 0.000000    Top5 0.000000    \r\n",
      "Test: [   60/  971]    Loss 11.947794    Top1 0.000000    Top5 0.250000    \r\n",
      "Test: [   90/  971]    Loss 11.966068    Top1 0.000000    Top5 0.333333    \r\n",
      "Test: [  120/  971]    Loss 11.978602    Top1 0.000000    Top5 0.333333    \r\n",
      "Test: [  150/  971]    Loss 11.984454    Top1 0.000000    Top5 0.366667    \r\n",
      "Test: [  180/  971]    Loss 12.000919    Top1 0.000000    Top5 0.333333    \r\n",
      "Test: [  210/  971]    Loss 12.007501    Top1 0.000000    Top5 0.357143    \r\n",
      "Test: [  240/  971]    Loss 12.016390    Top1 0.000000    Top5 0.312500    \r\n",
      "Test: [  270/  971]    Loss 11.998093    Top1 0.037037    Top5 0.351852    \r\n",
      "Test: [  300/  971]    Loss 12.022543    Top1 0.033333    Top5 0.333333    \r\n",
      "Test: [  330/  971]    Loss 12.027183    Top1 0.030303    Top5 0.303030    \r\n",
      "Test: [  360/  971]    Loss 12.029115    Top1 0.027778    Top5 0.305556    \r\n",
      "Test: [  390/  971]    Loss 12.048233    Top1 0.025641    Top5 0.320513    \r\n",
      "Test: [  420/  971]    Loss 12.046116    Top1 0.023810    Top5 0.321429    \r\n",
      "Test: [  450/  971]    Loss 12.044405    Top1 0.022222    Top5 0.300000    \r\n",
      "Test: [  480/  971]    Loss 12.023546    Top1 0.020833    Top5 0.302083    \r\n",
      "Test: [  510/  971]    Loss 12.023061    Top1 0.019608    Top5 0.313725    \r\n",
      "Test: [  540/  971]    Loss 12.023450    Top1 0.018519    Top5 0.333333    \r\n",
      "Test: [  570/  971]    Loss 12.030866    Top1 0.017544    Top5 0.350877    \r\n",
      "Test: [  600/  971]    Loss 12.027268    Top1 0.016667    Top5 0.358333    \r\n",
      "Test: [  630/  971]    Loss 12.022902    Top1 0.015873    Top5 0.357143    \r\n",
      "Test: [  660/  971]    Loss 12.026641    Top1 0.015152    Top5 0.348485    \r\n",
      "Test: [  690/  971]    Loss 12.027455    Top1 0.014493    Top5 0.347826    \r\n",
      "Test: [  720/  971]    Loss 12.021210    Top1 0.013889    Top5 0.340278    \r\n",
      "Test: [  750/  971]    Loss 12.010856    Top1 0.013333    Top5 0.353333    \r\n",
      "Test: [  780/  971]    Loss 12.013190    Top1 0.012821    Top5 0.346154    \r\n",
      "Test: [  810/  971]    Loss 12.012818    Top1 0.012346    Top5 0.339506    \r\n",
      "Test: [  840/  971]    Loss 12.017272    Top1 0.011905    Top5 0.333333    \r\n",
      "Test: [  870/  971]    Loss 12.016525    Top1 0.017241    Top5 0.339080    \r\n",
      "Test: [  900/  971]    Loss 12.014603    Top1 0.016667    Top5 0.333333    \r\n",
      "Test: [  930/  971]    Loss 12.014794    Top1 0.016129    Top5 0.344086    \r\n",
      "Test: [  960/  971]    Loss 12.022169    Top1 0.015625    Top5 0.338542    \r\n",
      "==> Top1: 0.015    Top5: 0.334    Loss: 12.023\r\n",
      "\r\n",
      "Test: top-1 = 0.015, top-5 = 0.334, loss = 12.023\r\n",
      "/home/bohumil/FIIT/BP/BP/Zdroje_kod/distiller/distiller/quantization/range_linear.py:894: UserWarning: Cannot retrieve clipping values because the activations aren't quantized.\r\n",
      "  warnings.warn(str(ex))   # probably the output isn't quantized\r\n",
      "\r\n",
      "Using \"Powell\" minimization algorithm.\r\n",
      "--- test ---------------------\r\n",
      "19439 samples (20 per mini-batch)\r\n",
      "Test: [   30/  971]    Loss 12.363800    Top1 0.000000    Top5 0.166667    \r\n",
      "Test: [   60/  971]    Loss 12.387967    Top1 0.000000    Top5 0.166667    \r\n",
      "Test: [   90/  971]    Loss 12.489467    Top1 0.000000    Top5 0.166667    \r\n",
      "Test: [  120/  971]    Loss 12.469914    Top1 0.000000    Top5 0.166667    \r\n",
      "Test: [  150/  971]    Loss 12.447859    Top1 0.000000    Top5 0.166667    \r\n",
      "Test: [  180/  971]    Loss 12.457983    Top1 0.000000    Top5 0.166667    \r\n",
      "Test: [  210/  971]    Loss 12.430690    Top1 0.000000    Top5 0.214286    \r\n",
      "Test: [  240/  971]    Loss 12.440624    Top1 0.020833    Top5 0.250000    \r\n",
      "Test: [  270/  971]    Loss 12.446241    Top1 0.018519    Top5 0.333333    \r\n",
      "Test: [  300/  971]    Loss 12.453962    Top1 0.016667    Top5 0.350000    \r\n",
      "Test: [  330/  971]    Loss 12.438504    Top1 0.015152    Top5 0.393939    \r\n",
      "Test: [  360/  971]    Loss 12.447457    Top1 0.027778    Top5 0.375000    \r\n",
      "Test: [  390/  971]    Loss 12.415506    Top1 0.038462    Top5 0.371795    \r\n",
      "Test: [  420/  971]    Loss 12.398226    Top1 0.035714    Top5 0.380952    \r\n",
      "Test: [  450/  971]    Loss 12.385012    Top1 0.044444    Top5 0.388889    \r\n",
      "Test: [  480/  971]    Loss 12.393485    Top1 0.041667    Top5 0.385417    \r\n",
      "Test: [  510/  971]    Loss 12.397909    Top1 0.039216    Top5 0.401961    \r\n",
      "Test: [  540/  971]    Loss 12.397904    Top1 0.037037    Top5 0.379630    \r\n",
      "Test: [  570/  971]    Loss 12.390785    Top1 0.035088    Top5 0.368421    \r\n",
      "Test: [  600/  971]    Loss 12.397134    Top1 0.033333    Top5 0.358333    \r\n",
      "Test: [  630/  971]    Loss 12.403847    Top1 0.039683    Top5 0.357143    \r\n",
      "Test: [  660/  971]    Loss 12.395349    Top1 0.037879    Top5 0.348485    \r\n",
      "Test: [  690/  971]    Loss 12.390780    Top1 0.036232    Top5 0.347826    \r\n",
      "Test: [  720/  971]    Loss 12.395269    Top1 0.041667    Top5 0.347222    \r\n",
      "Test: [  750/  971]    Loss 12.382466    Top1 0.040000    Top5 0.353333    \r\n",
      "Test: [  780/  971]    Loss 12.380077    Top1 0.038462    Top5 0.346154    \r\n",
      "Test: [  810/  971]    Loss 12.380222    Top1 0.037037    Top5 0.339506    \r\n",
      "Test: [  840/  971]    Loss 12.383338    Top1 0.035714    Top5 0.333333    \r\n",
      "Test: [  870/  971]    Loss 12.381025    Top1 0.034483    Top5 0.339080    \r\n",
      "Test: [  900/  971]    Loss 12.377702    Top1 0.033333    Top5 0.327778    \r\n",
      "Test: [  930/  971]    Loss 12.383295    Top1 0.032258    Top5 0.327957    \r\n",
      "Test: [  960/  971]    Loss 12.387437    Top1 0.031250    Top5 0.322917    \r\n",
      "==> Top1: 0.031    Top5: 0.324    Loss: 12.389\r\n",
      "\r\n",
      "--- test ---------------------\r\n",
      "19439 samples (20 per mini-batch)\r\n",
      "Test: [   30/  971]    Loss 12.311946    Top1 0.000000    Top5 0.333333    \r\n",
      "Test: [   60/  971]    Loss 12.372534    Top1 0.000000    Top5 0.583333    \r\n",
      "Test: [   90/  971]    Loss 12.392328    Top1 0.055556    Top5 0.500000    \r\n",
      "Test: [  120/  971]    Loss 12.401192    Top1 0.041667    Top5 0.458333    \r\n",
      "Test: [  150/  971]    Loss 12.393703    Top1 0.033333    Top5 0.400000    \r\n",
      "Test: [  180/  971]    Loss 12.404808    Top1 0.027778    Top5 0.361111    \r\n",
      "Test: [  210/  971]    Loss 12.413437    Top1 0.047619    Top5 0.380952    \r\n",
      "Test: [  240/  971]    Loss 12.395458    Top1 0.041667    Top5 0.333333    \r\n",
      "Test: [  270/  971]    Loss 12.437926    Top1 0.037037    Top5 0.296296    \r\n",
      "Test: [  300/  971]    Loss 12.430592    Top1 0.033333    Top5 0.316667    \r\n",
      "Test: [  330/  971]    Loss 12.441506    Top1 0.045455    Top5 0.318182    \r\n",
      "Test: [  360/  971]    Loss 12.430608    Top1 0.055556    Top5 0.333333    \r\n",
      "Test: [  390/  971]    Loss 12.408285    Top1 0.051282    Top5 0.346154    \r\n",
      "Test: [  420/  971]    Loss 12.419870    Top1 0.047619    Top5 0.333333    \r\n",
      "Test: [  450/  971]    Loss 12.413566    Top1 0.055556    Top5 0.355556    \r\n",
      "Test: [  480/  971]    Loss 12.415865    Top1 0.052083    Top5 0.343750    \r\n",
      "Test: [  510/  971]    Loss 12.423434    Top1 0.049020    Top5 0.352941    \r\n",
      "Test: [  540/  971]    Loss 12.417305    Top1 0.046296    Top5 0.361111    \r\n",
      "Test: [  570/  971]    Loss 12.423604    Top1 0.043860    Top5 0.385965    \r\n",
      "Test: [  600/  971]    Loss 12.417909    Top1 0.041667    Top5 0.375000    \r\n",
      "Test: [  630/  971]    Loss 12.412630    Top1 0.039683    Top5 0.357143    \r\n",
      "Test: [  660/  971]    Loss 12.402489    Top1 0.037879    Top5 0.356061    \r\n",
      "Test: [  690/  971]    Loss 12.394674    Top1 0.036232    Top5 0.355072    \r\n",
      "Test: [  720/  971]    Loss 12.392227    Top1 0.034722    Top5 0.347222    \r\n",
      "Test: [  750/  971]    Loss 12.388054    Top1 0.033333    Top5 0.340000    \r\n",
      "Test: [  780/  971]    Loss 12.386708    Top1 0.032051    Top5 0.346154    \r\n",
      "Test: [  810/  971]    Loss 12.381875    Top1 0.030864    Top5 0.333333    \r\n",
      "Test: [  840/  971]    Loss 12.393774    Top1 0.029762    Top5 0.339286    \r\n",
      "Test: [  870/  971]    Loss 12.387716    Top1 0.034483    Top5 0.344828    \r\n",
      "Test: [  900/  971]    Loss 12.387858    Top1 0.033333    Top5 0.333333    \r\n",
      "Test: [  930/  971]    Loss 12.390049    Top1 0.032258    Top5 0.333333    \r\n",
      "Test: [  960/  971]    Loss 12.389312    Top1 0.031250    Top5 0.328125    \r\n",
      "==> Top1: 0.031    Top5: 0.324    Loss: 12.389\r\n",
      "\r\n",
      "--- test ---------------------\r\n",
      "19439 samples (20 per mini-batch)\r\n",
      "Test: [   30/  971]    Loss 12.793862    Top1 0.000000    Top5 0.500000    \r\n",
      "Test: [   60/  971]    Loss 12.862333    Top1 0.000000    Top5 0.666667    \r\n",
      "Test: [   90/  971]    Loss 12.874215    Top1 0.055556    Top5 0.611111    \r\n",
      "Test: [  120/  971]    Loss 12.809814    Top1 0.083333    Top5 0.791667    \r\n",
      "Test: [  150/  971]    Loss 12.805599    Top1 0.100000    Top5 0.700000    \r\n",
      "Test: [  180/  971]    Loss 12.823141    Top1 0.083333    Top5 0.666667    \r\n",
      "Test: [  210/  971]    Loss 12.804738    Top1 0.095238    Top5 0.595238    \r\n",
      "Test: [  240/  971]    Loss 12.781271    Top1 0.083333    Top5 0.645833    \r\n",
      "Test: [  270/  971]    Loss 12.788877    Top1 0.074074    Top5 0.611111    \r\n",
      "Test: [  300/  971]    Loss 12.809604    Top1 0.083333    Top5 0.633333    \r\n",
      "Test: [  330/  971]    Loss 12.819404    Top1 0.075758    Top5 0.575758    \r\n",
      "Test: [  360/  971]    Loss 12.811746    Top1 0.069444    Top5 0.541667    \r\n",
      "Test: [  390/  971]    Loss 12.812005    Top1 0.064103    Top5 0.500000    \r\n",
      "Test: [  420/  971]    Loss 12.821276    Top1 0.071429    Top5 0.523810    \r\n",
      "Test: [  450/  971]    Loss 12.822585    Top1 0.077778    Top5 0.522222    \r\n",
      "Test: [  480/  971]    Loss 12.830325    Top1 0.083333    Top5 0.531250    \r\n",
      "Test: [  510/  971]    Loss 12.835492    Top1 0.088235    Top5 0.519608    \r\n",
      "Test: [  540/  971]    Loss 12.841949    Top1 0.092593    Top5 0.518519    \r\n",
      "Test: [  570/  971]    Loss 12.832870    Top1 0.087719    Top5 0.491228    \r\n",
      "Test: [  600/  971]    Loss 12.842363    Top1 0.083333    Top5 0.491667    \r\n",
      "Test: [  630/  971]    Loss 12.843822    Top1 0.079365    Top5 0.476190    \r\n",
      "Test: [  660/  971]    Loss 12.851226    Top1 0.075758    Top5 0.469697    \r\n",
      "Test: [  690/  971]    Loss 12.853937    Top1 0.079710    Top5 0.478261    \r\n",
      "Test: [  720/  971]    Loss 12.863567    Top1 0.076389    Top5 0.472222    \r\n",
      "Test: [  750/  971]    Loss 12.868664    Top1 0.080000    Top5 0.473333    \r\n",
      "Test: [  780/  971]    Loss 12.869015    Top1 0.083333    Top5 0.474359    \r\n",
      "Test: [  810/  971]    Loss 12.878268    Top1 0.080247    Top5 0.462963    \r\n",
      "Test: [  840/  971]    Loss 12.881721    Top1 0.077381    Top5 0.452381    \r\n",
      "Test: [  870/  971]    Loss 12.877604    Top1 0.074713    Top5 0.448276    \r\n",
      "Test: [  900/  971]    Loss 12.878504    Top1 0.083333    Top5 0.455556    \r\n",
      "Test: [  930/  971]    Loss 12.882157    Top1 0.080645    Top5 0.451613    \r\n",
      "Test: [  960/  971]    Loss 12.877730    Top1 0.078125    Top5 0.453125    \r\n",
      "==> Top1: 0.082    Top5: 0.453    Loss: 12.877\r\n",
      "\r\n",
      "--- test ---------------------\r\n",
      "19439 samples (20 per mini-batch)\r\n",
      "Test: [   30/  971]    Loss 12.317577    Top1 0.000000    Top5 0.666667    \r\n",
      "Test: [   60/  971]    Loss 12.408731    Top1 0.083333    Top5 0.500000    \r\n",
      "Test: [   90/  971]    Loss 12.456667    Top1 0.055556    Top5 0.500000    \r\n",
      "Test: [  120/  971]    Loss 12.495682    Top1 0.083333    Top5 0.583333    \r\n",
      "Test: [  150/  971]    Loss 12.500675    Top1 0.066667    Top5 0.533333    \r\n",
      "Test: [  180/  971]    Loss 12.495581    Top1 0.055556    Top5 0.555556    \r\n",
      "Test: [  210/  971]    Loss 12.489647    Top1 0.047619    Top5 0.547619    \r\n",
      "Test: [  240/  971]    Loss 12.484735    Top1 0.041667    Top5 0.541667    \r\n",
      "Test: [  270/  971]    Loss 12.512173    Top1 0.037037    Top5 0.481481    \r\n",
      "Test: [  300/  971]    Loss 12.538010    Top1 0.033333    Top5 0.450000    \r\n",
      "Test: [  330/  971]    Loss 12.569087    Top1 0.045455    Top5 0.424242    \r\n",
      "Test: [  360/  971]    Loss 12.565296    Top1 0.041667    Top5 0.430556    \r\n",
      "Test: [  390/  971]    Loss 12.556243    Top1 0.051282    Top5 0.474359    \r\n",
      "Test: [  420/  971]    Loss 12.548584    Top1 0.059524    Top5 0.488095    \r\n",
      "Test: [  450/  971]    Loss 12.555886    Top1 0.055556    Top5 0.466667    \r\n",
      "Test: [  480/  971]    Loss 12.537853    Top1 0.072917    Top5 0.458333    \r\n",
      "Test: [  510/  971]    Loss 12.530843    Top1 0.088235    Top5 0.490196    \r\n",
      "Test: [  540/  971]    Loss 12.524293    Top1 0.083333    Top5 0.472222    \r\n",
      "Test: [  570/  971]    Loss 12.522846    Top1 0.096491    Top5 0.500000    \r\n",
      "Test: [  600/  971]    Loss 12.521281    Top1 0.100000    Top5 0.508333    \r\n",
      "Test: [  630/  971]    Loss 12.532483    Top1 0.095238    Top5 0.484127    \r\n",
      "Test: [  660/  971]    Loss 12.537502    Top1 0.090909    Top5 0.469697    \r\n",
      "Test: [  690/  971]    Loss 12.535838    Top1 0.101449    Top5 0.485507    \r\n",
      "Test: [  720/  971]    Loss 12.547307    Top1 0.097222    Top5 0.479167    \r\n",
      "Test: [  750/  971]    Loss 12.542495    Top1 0.093333    Top5 0.506667    \r\n",
      "Test: [  780/  971]    Loss 12.546497    Top1 0.089744    Top5 0.500000    \r\n",
      "Test: [  810/  971]    Loss 12.544352    Top1 0.086420    Top5 0.512346    \r\n",
      "Test: [  840/  971]    Loss 12.537381    Top1 0.089286    Top5 0.517857    \r\n",
      "Test: [  870/  971]    Loss 12.544285    Top1 0.086207    Top5 0.500000    \r\n",
      "Test: [  900/  971]    Loss 12.534146    Top1 0.083333    Top5 0.505556    \r\n",
      "Test: [  930/  971]    Loss 12.531340    Top1 0.080645    Top5 0.489247    \r\n",
      "Test: [  960/  971]    Loss 12.533055    Top1 0.078125    Top5 0.473958    \r\n",
      "==> Top1: 0.077    Top5: 0.468    Loss: 12.533\r\n",
      "\r\n",
      "--- test ---------------------\r\n",
      "19439 samples (20 per mini-batch)\r\n",
      "Test: [   30/  971]    Loss 12.407060    Top1 0.166667    Top5 0.500000    \r\n",
      "Test: [   60/  971]    Loss 12.503432    Top1 0.083333    Top5 0.416667    \r\n",
      "Test: [   90/  971]    Loss 12.482465    Top1 0.055556    Top5 0.333333    \r\n",
      "Test: [  120/  971]    Loss 12.460194    Top1 0.041667    Top5 0.291667    \r\n",
      "Test: [  150/  971]    Loss 12.476934    Top1 0.033333    Top5 0.233333    \r\n",
      "Test: [  180/  971]    Loss 12.447605    Top1 0.027778    Top5 0.250000    \r\n",
      "Test: [  210/  971]    Loss 12.450105    Top1 0.023810    Top5 0.261905    \r\n",
      "Test: [  240/  971]    Loss 12.450715    Top1 0.020833    Top5 0.291667    \r\n",
      "Test: [  270/  971]    Loss 12.438286    Top1 0.018519    Top5 0.351852    \r\n",
      "Test: [  300/  971]    Loss 12.439319    Top1 0.016667    Top5 0.366667    \r\n",
      "Test: [  330/  971]    Loss 12.413980    Top1 0.015152    Top5 0.333333    \r\n",
      "Test: [  360/  971]    Loss 12.393395    Top1 0.027778    Top5 0.375000    \r\n",
      "Test: [  390/  971]    Loss 12.389359    Top1 0.025641    Top5 0.358974    \r\n",
      "Test: [  420/  971]    Loss 12.377756    Top1 0.035714    Top5 0.357143    \r\n",
      "Test: [  450/  971]    Loss 12.371226    Top1 0.033333    Top5 0.333333    \r\n",
      "Test: [  480/  971]    Loss 12.376820    Top1 0.031250    Top5 0.312500    \r\n",
      "Test: [  510/  971]    Loss 12.386127    Top1 0.029412    Top5 0.333333    \r\n",
      "Test: [  540/  971]    Loss 12.373393    Top1 0.027778    Top5 0.342593    \r\n",
      "Test: [  570/  971]    Loss 12.378631    Top1 0.026316    Top5 0.333333    \r\n",
      "Test: [  600/  971]    Loss 12.371820    Top1 0.033333    Top5 0.325000    \r\n",
      "Test: [  630/  971]    Loss 12.372609    Top1 0.031746    Top5 0.325397    \r\n",
      "Test: [  660/  971]    Loss 12.373337    Top1 0.037879    Top5 0.318182    \r\n",
      "Test: [  690/  971]    Loss 12.377978    Top1 0.043478    Top5 0.333333    \r\n",
      "Test: [  720/  971]    Loss 12.383903    Top1 0.041667    Top5 0.333333    \r\n",
      "Test: [  750/  971]    Loss 12.383464    Top1 0.040000    Top5 0.333333    \r\n",
      "Test: [  780/  971]    Loss 12.384159    Top1 0.038462    Top5 0.326923    \r\n",
      "Test: [  810/  971]    Loss 12.392114    Top1 0.037037    Top5 0.327160    \r\n",
      "Test: [  840/  971]    Loss 12.392998    Top1 0.035714    Top5 0.333333    \r\n",
      "Test: [  870/  971]    Loss 12.387996    Top1 0.034483    Top5 0.344828    \r\n",
      "Test: [  900/  971]    Loss 12.382598    Top1 0.033333    Top5 0.333333    \r\n",
      "Test: [  930/  971]    Loss 12.388625    Top1 0.032258    Top5 0.327957    \r\n",
      "Test: [  960/  971]    Loss 12.390284    Top1 0.031250    Top5 0.322917    \r\n",
      "==> Top1: 0.031    Top5: 0.324    Loss: 12.389\r\n",
      "\r\n",
      "--- test ---------------------\r\n",
      "19439 samples (20 per mini-batch)\r\n",
      "Test: [   30/  971]    Loss 12.214549    Top1 0.000000    Top5 0.000000    \r\n",
      "Test: [   60/  971]    Loss 12.215272    Top1 0.000000    Top5 0.083333    \r\n",
      "Test: [   90/  971]    Loss 12.223571    Top1 0.000000    Top5 0.055556    \r\n",
      "Test: [  120/  971]    Loss 12.172670    Top1 0.041667    Top5 0.125000    \r\n",
      "Test: [  150/  971]    Loss 12.183816    Top1 0.033333    Top5 0.133333    \r\n",
      "Test: [  180/  971]    Loss 12.180635    Top1 0.027778    Top5 0.138889    \r\n",
      "Test: [  210/  971]    Loss 12.220317    Top1 0.023810    Top5 0.142857    \r\n",
      "Test: [  240/  971]    Loss 12.223770    Top1 0.020833    Top5 0.166667    \r\n",
      "Test: [  270/  971]    Loss 12.222903    Top1 0.018519    Top5 0.185185    \r\n",
      "Test: [  300/  971]    Loss 12.213555    Top1 0.016667    Top5 0.183333    \r\n",
      "Test: [  330/  971]    Loss 12.202657    Top1 0.015152    Top5 0.181818    \r\n",
      "Test: [  360/  971]    Loss 12.206275    Top1 0.013889    Top5 0.180556    \r\n",
      "Test: [  390/  971]    Loss 12.215203    Top1 0.012821    Top5 0.179487    \r\n",
      "Test: [  420/  971]    Loss 12.224846    Top1 0.011905    Top5 0.166667    \r\n",
      "Test: [  450/  971]    Loss 12.232833    Top1 0.011111    Top5 0.166667    \r\n",
      "Test: [  480/  971]    Loss 12.243949    Top1 0.010417    Top5 0.166667    \r\n",
      "Test: [  510/  971]    Loss 12.235077    Top1 0.009804    Top5 0.156863    \r\n",
      "Test: [  540/  971]    Loss 12.243515    Top1 0.009259    Top5 0.148148    \r\n",
      "Test: [  570/  971]    Loss 12.241863    Top1 0.008772    Top5 0.149123    \r\n",
      "Test: [  600/  971]    Loss 12.244899    Top1 0.008333    Top5 0.141667    \r\n",
      "Test: [  630/  971]    Loss 12.250892    Top1 0.007937    Top5 0.142857    \r\n",
      "Test: [  660/  971]    Loss 12.250335    Top1 0.007576    Top5 0.136364    \r\n",
      "Test: [  690/  971]    Loss 12.249876    Top1 0.007246    Top5 0.137681    \r\n",
      "Test: [  720/  971]    Loss 12.247604    Top1 0.006944    Top5 0.145833    \r\n",
      "Test: [  750/  971]    Loss 12.251718    Top1 0.006667    Top5 0.140000    \r\n",
      "Test: [  780/  971]    Loss 12.251471    Top1 0.006410    Top5 0.147436    \r\n",
      "Test: [  810/  971]    Loss 12.255966    Top1 0.006173    Top5 0.154321    \r\n",
      "Test: [  840/  971]    Loss 12.257918    Top1 0.005952    Top5 0.154762    \r\n",
      "Test: [  870/  971]    Loss 12.264739    Top1 0.005747    Top5 0.155172    \r\n",
      "Test: [  900/  971]    Loss 12.262793    Top1 0.005556    Top5 0.150000    \r\n",
      "Test: [  930/  971]    Loss 12.263832    Top1 0.005376    Top5 0.150538    \r\n",
      "Test: [  960/  971]    Loss 12.265735    Top1 0.005208    Top5 0.156250    \r\n",
      "==> Top1: 0.005    Top5: 0.154    Loss: 12.266\r\n",
      "\r\n",
      "--- test ---------------------\r\n",
      "19439 samples (20 per mini-batch)\r\n",
      "Test: [   30/  971]    Loss 12.240845    Top1 0.000000    Top5 0.500000    \r\n",
      "Test: [   60/  971]    Loss 12.278177    Top1 0.000000    Top5 0.333333    \r\n",
      "Test: [   90/  971]    Loss 12.252335    Top1 0.000000    Top5 0.388889    \r\n",
      "Test: [  120/  971]    Loss 12.295000    Top1 0.041667    Top5 0.375000    \r\n",
      "Test: [  150/  971]    Loss 12.341665    Top1 0.033333    Top5 0.366667    \r\n",
      "Test: [  180/  971]    Loss 12.351271    Top1 0.027778    Top5 0.333333    \r\n",
      "Test: [  210/  971]    Loss 12.366140    Top1 0.023810    Top5 0.333333    \r\n",
      "Test: [  240/  971]    Loss 12.372992    Top1 0.020833    Top5 0.312500    \r\n",
      "Test: [  270/  971]    Loss 12.365779    Top1 0.018519    Top5 0.277778    \r\n",
      "Test: [  300/  971]    Loss 12.361454    Top1 0.016667    Top5 0.283333    \r\n",
      "Test: [  330/  971]    Loss 12.343570    Top1 0.060606    Top5 0.378788    \r\n",
      "Test: [  360/  971]    Loss 12.361536    Top1 0.069444    Top5 0.402778    \r\n",
      "Test: [  390/  971]    Loss 12.364180    Top1 0.064103    Top5 0.384615    \r\n",
      "Test: [  420/  971]    Loss 12.358845    Top1 0.083333    Top5 0.392857    \r\n",
      "Test: [  450/  971]    Loss 12.347583    Top1 0.088889    Top5 0.388889    \r\n",
      "Test: [  480/  971]    Loss 12.346698    Top1 0.083333    Top5 0.395833    \r\n",
      "Test: [  510/  971]    Loss 12.336023    Top1 0.088235    Top5 0.441176    \r\n",
      "Test: [  540/  971]    Loss 12.328834    Top1 0.092593    Top5 0.444444    \r\n",
      "Test: [  570/  971]    Loss 12.315870    Top1 0.087719    Top5 0.473684    \r\n",
      "Test: [  600/  971]    Loss 12.303338    Top1 0.083333    Top5 0.475000    \r\n",
      "Test: [  630/  971]    Loss 12.298693    Top1 0.079365    Top5 0.476190    \r\n",
      "Test: [  660/  971]    Loss 12.295248    Top1 0.083333    Top5 0.469697    \r\n",
      "Test: [  690/  971]    Loss 12.292774    Top1 0.086957    Top5 0.463768    \r\n",
      "Test: [  720/  971]    Loss 12.304073    Top1 0.090278    Top5 0.458333    \r\n",
      "Test: [  750/  971]    Loss 12.299483    Top1 0.093333    Top5 0.453333    \r\n",
      "Test: [  780/  971]    Loss 12.307611    Top1 0.096154    Top5 0.455128    \r\n",
      "Test: [  810/  971]    Loss 12.301116    Top1 0.098765    Top5 0.456790    \r\n",
      "Test: [  840/  971]    Loss 12.296505    Top1 0.095238    Top5 0.440476    \r\n",
      "Test: [  870/  971]    Loss 12.296864    Top1 0.097701    Top5 0.442529    \r\n",
      "Test: [  900/  971]    Loss 12.301568    Top1 0.105556    Top5 0.455556    \r\n",
      "Test: [  930/  971]    Loss 12.305479    Top1 0.112903    Top5 0.462366    \r\n",
      "Test: [  960/  971]    Loss 12.310128    Top1 0.109375    Top5 0.453125    \r\n",
      "==> Top1: 0.108    Top5: 0.448    Loss: 12.311\r\n",
      "\r\n",
      "--- test ---------------------\r\n",
      "19439 samples (20 per mini-batch)\r\n",
      "Test: [   30/  971]    Loss 12.340328    Top1 0.000000    Top5 0.000000    \r\n",
      "Test: [   60/  971]    Loss 12.210680    Top1 0.000000    Top5 0.083333    \r\n",
      "Test: [   90/  971]    Loss 12.220997    Top1 0.055556    Top5 0.111111    \r\n",
      "Test: [  120/  971]    Loss 12.232978    Top1 0.041667    Top5 0.083333    \r\n",
      "Test: [  150/  971]    Loss 12.193650    Top1 0.033333    Top5 0.133333    \r\n",
      "Test: [  180/  971]    Loss 12.254454    Top1 0.027778    Top5 0.111111    \r\n",
      "Test: [  210/  971]    Loss 12.273624    Top1 0.023810    Top5 0.142857    \r\n",
      "Test: [  240/  971]    Loss 12.268882    Top1 0.020833    Top5 0.125000    \r\n",
      "Test: [  270/  971]    Loss 12.264666    Top1 0.018519    Top5 0.111111    \r\n",
      "Test: [  300/  971]    Loss 12.251198    Top1 0.016667    Top5 0.133333    \r\n",
      "Test: [  330/  971]    Loss 12.258244    Top1 0.015152    Top5 0.136364    \r\n",
      "Test: [  360/  971]    Loss 12.256725    Top1 0.013889    Top5 0.125000    \r\n",
      "Test: [  390/  971]    Loss 12.260898    Top1 0.012821    Top5 0.115385    \r\n",
      "Test: [  420/  971]    Loss 12.242886    Top1 0.011905    Top5 0.130952    \r\n",
      "Test: [  450/  971]    Loss 12.223059    Top1 0.011111    Top5 0.144444    \r\n",
      "Test: [  480/  971]    Loss 12.209712    Top1 0.010417    Top5 0.135417    \r\n",
      "Test: [  510/  971]    Loss 12.209559    Top1 0.009804    Top5 0.127451    \r\n",
      "Test: [  540/  971]    Loss 12.205158    Top1 0.009259    Top5 0.120370    \r\n",
      "Test: [  570/  971]    Loss 12.185987    Top1 0.008772    Top5 0.140351    \r\n",
      "Test: [  600/  971]    Loss 12.185063    Top1 0.008333    Top5 0.141667    \r\n",
      "Test: [  630/  971]    Loss 12.177539    Top1 0.007937    Top5 0.142857    \r\n",
      "Test: [  660/  971]    Loss 12.179563    Top1 0.007576    Top5 0.136364    \r\n",
      "Test: [  690/  971]    Loss 12.181364    Top1 0.007246    Top5 0.137681    \r\n",
      "Test: [  720/  971]    Loss 12.185393    Top1 0.006944    Top5 0.138889    \r\n",
      "Test: [  750/  971]    Loss 12.181200    Top1 0.006667    Top5 0.133333    \r\n",
      "Test: [  780/  971]    Loss 12.177095    Top1 0.006410    Top5 0.134615    \r\n",
      "Test: [  810/  971]    Loss 12.172693    Top1 0.006173    Top5 0.135802    \r\n",
      "Test: [  840/  971]    Loss 12.171246    Top1 0.005952    Top5 0.130952    \r\n",
      "Test: [  870/  971]    Loss 12.166680    Top1 0.005747    Top5 0.132184    \r\n",
      "Test: [  900/  971]    Loss 12.161032    Top1 0.005556    Top5 0.138889    \r\n",
      "Test: [  930/  971]    Loss 12.153719    Top1 0.005376    Top5 0.134409    \r\n",
      "Test: [  960/  971]    Loss 12.150193    Top1 0.005208    Top5 0.130208    \r\n",
      "==> Top1: 0.010    Top5: 0.139    Loss: 12.150\r\n",
      "\r\n",
      "--- test ---------------------\r\n",
      "19439 samples (20 per mini-batch)\r\n",
      "Test: [   30/  971]    Loss 12.190841    Top1 0.000000    Top5 0.666667    \r\n",
      "Test: [   60/  971]    Loss 12.217480    Top1 0.000000    Top5 0.583333    \r\n",
      "Test: [   90/  971]    Loss 12.325150    Top1 0.000000    Top5 0.388889    \r\n",
      "Test: [  120/  971]    Loss 12.288897    Top1 0.000000    Top5 0.333333    \r\n",
      "Test: [  150/  971]    Loss 12.281321    Top1 0.000000    Top5 0.266667    \r\n",
      "Test: [  180/  971]    Loss 12.273583    Top1 0.000000    Top5 0.277778    \r\n",
      "Test: [  210/  971]    Loss 12.315256    Top1 0.000000    Top5 0.261905    \r\n",
      "Test: [  240/  971]    Loss 12.306707    Top1 0.000000    Top5 0.270833    \r\n",
      "Test: [  270/  971]    Loss 12.328317    Top1 0.018519    Top5 0.296296    \r\n",
      "Test: [  300/  971]    Loss 12.321627    Top1 0.016667    Top5 0.283333    \r\n",
      "Test: [  330/  971]    Loss 12.334511    Top1 0.015152    Top5 0.303030    \r\n",
      "Test: [  360/  971]    Loss 12.330193    Top1 0.013889    Top5 0.319444    \r\n",
      "Test: [  390/  971]    Loss 12.331956    Top1 0.025641    Top5 0.320513    \r\n",
      "Test: [  420/  971]    Loss 12.345260    Top1 0.035714    Top5 0.321429    \r\n",
      "Test: [  450/  971]    Loss 12.330982    Top1 0.033333    Top5 0.311111    \r\n",
      "Test: [  480/  971]    Loss 12.320773    Top1 0.031250    Top5 0.364583    \r\n",
      "Test: [  510/  971]    Loss 12.328601    Top1 0.029412    Top5 0.362745    \r\n",
      "Test: [  540/  971]    Loss 12.326485    Top1 0.027778    Top5 0.342593    \r\n",
      "Test: [  570/  971]    Loss 12.319425    Top1 0.035088    Top5 0.350877    \r\n",
      "Test: [  600/  971]    Loss 12.315977    Top1 0.033333    Top5 0.350000    \r\n",
      "Test: [  630/  971]    Loss 12.319038    Top1 0.031746    Top5 0.341270    \r\n",
      "Test: [  660/  971]    Loss 12.322876    Top1 0.030303    Top5 0.325758    \r\n",
      "Test: [  690/  971]    Loss 12.316960    Top1 0.043478    Top5 0.340580    \r\n",
      "Test: [  720/  971]    Loss 12.320882    Top1 0.041667    Top5 0.333333    \r\n",
      "Test: [  750/  971]    Loss 12.316636    Top1 0.040000    Top5 0.320000    \r\n",
      "Test: [  780/  971]    Loss 12.311464    Top1 0.038462    Top5 0.326923    \r\n",
      "Test: [  810/  971]    Loss 12.314980    Top1 0.037037    Top5 0.320988    \r\n",
      "Test: [  840/  971]    Loss 12.318694    Top1 0.035714    Top5 0.327381    \r\n",
      "Test: [  870/  971]    Loss 12.311340    Top1 0.034483    Top5 0.327586    \r\n",
      "Test: [  900/  971]    Loss 12.307398    Top1 0.033333    Top5 0.333333    \r\n",
      "Test: [  930/  971]    Loss 12.304672    Top1 0.032258    Top5 0.333333    \r\n",
      "Test: [  960/  971]    Loss 12.308493    Top1 0.031250    Top5 0.343750    \r\n",
      "==> Top1: 0.031    Top5: 0.340    Loss: 12.309\r\n",
      "\r\n",
      "--- test ---------------------\r\n",
      "19439 samples (20 per mini-batch)\r\n",
      "Test: [   30/  971]    Loss 12.441494    Top1 0.000000    Top5 0.000000    \r\n",
      "Test: [   60/  971]    Loss 12.431872    Top1 0.000000    Top5 0.000000    \r\n",
      "Test: [   90/  971]    Loss 12.388971    Top1 0.000000    Top5 0.111111    \r\n",
      "Test: [  120/  971]    Loss 12.368311    Top1 0.000000    Top5 0.125000    \r\n",
      "Test: [  150/  971]    Loss 12.388225    Top1 0.000000    Top5 0.166667    \r\n",
      "Test: [  180/  971]    Loss 12.373481    Top1 0.027778    Top5 0.194444    \r\n",
      "Test: [  210/  971]    Loss 12.413926    Top1 0.023810    Top5 0.238095    \r\n",
      "Test: [  240/  971]    Loss 12.405732    Top1 0.041667    Top5 0.270833    \r\n",
      "Test: [  270/  971]    Loss 12.376724    Top1 0.037037    Top5 0.314815    \r\n",
      "Test: [  300/  971]    Loss 12.365324    Top1 0.033333    Top5 0.316667    \r\n",
      "Test: [  330/  971]    Loss 12.362230    Top1 0.030303    Top5 0.287879    \r\n",
      "Test: [  360/  971]    Loss 12.346715    Top1 0.027778    Top5 0.291667    \r\n",
      "Test: [  390/  971]    Loss 12.354364    Top1 0.025641    Top5 0.282051    \r\n",
      "Test: [  420/  971]    Loss 12.353354    Top1 0.023810    Top5 0.273810    \r\n",
      "Test: [  450/  971]    Loss 12.354356    Top1 0.022222    Top5 0.277778    \r\n",
      "Test: [  480/  971]    Loss 12.357713    Top1 0.020833    Top5 0.270833    \r\n",
      "Test: [  510/  971]    Loss 12.359013    Top1 0.019608    Top5 0.264706    \r\n",
      "Test: [  540/  971]    Loss 12.361656    Top1 0.018519    Top5 0.287037    \r\n",
      "Test: [  570/  971]    Loss 12.352112    Top1 0.017544    Top5 0.280702    \r\n",
      "Test: [  600/  971]    Loss 12.362003    Top1 0.016667    Top5 0.266667    \r\n",
      "Test: [  630/  971]    Loss 12.351865    Top1 0.023810    Top5 0.269841    \r\n",
      "Test: [  660/  971]    Loss 12.361402    Top1 0.022727    Top5 0.257576    \r\n",
      "Test: [  690/  971]    Loss 12.349280    Top1 0.021739    Top5 0.253623    \r\n",
      "Test: [  720/  971]    Loss 12.358256    Top1 0.020833    Top5 0.243056    \r\n",
      "^C\r\n",
      "\r\n",
      "-- KeyboardInterrupt --\r\n",
      "\r\n",
      "Log file for this run: /home/bohumil/FIIT/BP/BP/Zdroje_kod/quantization_jupyters/resnet/logs/2020.04.23-135409/2020.04.23-135409.log\r\n"
     ]
    }
   ],
   "source": [
    "# ! python /home/bohumil/FIIT/BP/BP/Zdroje_kod/distiller/examples/classifier_compression/compress_classifier.py --arch resnet18 --pretrained --quantize-eval --qe-config-file /home/bohumil/FIIT/BP/BP/Zdroje_kod/distiller/examples/quantization/post_train_quant/resnet18_imagenet_post_train_lapq.yaml --validation-split 0.3  -j 2 -b 20 --eval /home/bohumil/FIIT/BP/BP/Zdroje_kod/imagenet/train\n",
    "! python /home/bohumil/FIIT/BP/BP/Zdroje_kod/distiller/examples/classifier_compression/compress_classifier.py --arch resnet18 --print-freq 30 -b 20 -j 1 --pretrained --validation-split 0.2 --eval --quantize-eval --qe-config-file /home/bohumil/FIIT/BP/BP/Zdroje_kod/distiller/examples/quantization/post_train_quant/resnet18_imagenet_post_train_lapq.yaml --qe-lapq --lapq-search-clipping /home/bohumil/FIIT/BP/BP/Zdroje_kod/imagenet/"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log file for this run: /home/bohumil/FIIT/BP/BP/Zdroje_kod/quantization_jupyters/resnet/logs/2020.04.23-150406/2020.04.23-150406.log\r\n",
      "Random seed: 0\r\n",
      "\r\n",
      "--------------------------------------------------------\r\n",
      "Logging to TensorBoard - remember to execute the server:\r\n",
      "> tensorboard --logdir='./logs'\r\n",
      "\r\n",
      "=> created a pretrained resnet18 model with the imagenet dataset\r\n",
      "Dataset sizes:\r\n",
      "\ttest=194\r\n",
      "Dataset sizes:\r\n",
      "\ttest=19439\r\n",
      "Reading configuration from: /home/bohumil/FIIT/BP/BP/Zdroje_kod/distiller/examples/quantization/post_train_quant/resnet18_imagenet_post_train_lapq.yaml\r\n",
      "Found component of class PostTrainLinearQuantizer: Name: post_train_quantizer ; Section: quantizers\r\n",
      "Loading activation stats from: /home/bohumil/FIIT/BP/BP/Zdroje_kod/distiller/examples/quantization/post_train_quant/stats/resnet18_quant_stats.yaml\r\n",
      "Initializing quantizer...\r\n",
      "Preparing model for quantization using PostTrainLinearQuantizer\r\n",
      "Applying batch-norm folding ahead of post-training quantization\r\n",
      "Propagating output statistics from BN modules to folded modules\r\n",
      "Optimizing output statistics for modules followed by ReLU/Tanh/Sigmoid\r\n",
      "Updated stats saved to logs/2020.04.23-150406/quant_stats_after_prepare_model.yaml\r\n",
      "Per-layer quantization parameters saved to logs/2020.04.23-150406/layer_quant_params.yaml\r\n",
      "Initializing quantization parameters...\r\n",
      "module.layer1.0.conv1\r\n",
      "\r\n",
      "Log file for this run: /home/bohumil/FIIT/BP/BP/Zdroje_kod/quantization_jupyters/resnet/logs/2020.04.23-150406/2020.04.23-150406.log\r\n",
      "Traceback (most recent call last):\r\n",
      "  File \"/home/bohumil/FIIT/BP/BP/Zdroje_kod/distiller/examples/classifier_compression/compress_classifier.py\", line 212, in <module>\r\n",
      "    main()\r\n",
      "  File \"/home/bohumil/FIIT/BP/BP/Zdroje_kod/distiller/examples/classifier_compression/compress_classifier.py\", line 75, in main\r\n",
      "    if app.handle_subapps():\r\n",
      "  File \"/home/bohumil/FIIT/BP/BP/Zdroje_kod/distiller/examples/classifier_compression/compress_classifier.py\", line 175, in handle_subapps\r\n",
      "    self.compression_scheduler, self.pylogger, self.args)\r\n",
      "  File \"/home/bohumil/FIIT/BP/BP/Zdroje_kod/distiller/examples/classifier_compression/compress_classifier.py\", line 115, in handle_subapps\r\n",
      "    image_classifier_ptq_lapq(model, criterion, pylogger, args)\r\n",
      "  File \"/home/bohumil/FIIT/BP/BP/Zdroje_kod/distiller/examples/classifier_compression/ptq_lapq.py\", line 82, in image_classifier_ptq_lapq\r\n",
      "    **lapq.cmdline_args_to_dict(args))\r\n",
      "  File \"/home/bohumil/FIIT/BP/BP/Zdroje_kod/distiller/distiller/quantization/ptq_coordinate_search.py\", line 393, in ptq_coordinate_search\r\n",
      "    search_clipping=search_clipping)\r\n",
      "  File \"/home/bohumil/FIIT/BP/BP/Zdroje_kod/distiller/distiller/quantization/ptq_coordinate_search.py\", line 255, in init_linear_quant_params\r\n",
      "    search_clipping=search_clipping)\r\n",
      "  File \"/home/bohumil/FIIT/BP/BP/Zdroje_kod/distiller/distiller/quantization/ptq_coordinate_search.py\", line 217, in init_layer_linear_quant_params\r\n",
      "    input_for_layer = get_input_for_layer(original_model, layer_name, eval_fn)\r\n",
      "  File \"/home/bohumil/FIIT/BP/BP/Zdroje_kod/distiller/distiller/quantization/ptq_coordinate_search.py\", line 170, in get_input_for_layer\r\n",
      "    eval_fn(model)\r\n",
      "  File \"/home/bohumil/FIIT/BP/BP/Zdroje_kod/distiller/examples/classifier_compression/ptq_lapq.py\", line 59, in eval_fn\r\n",
      "    outputs = model(images)\r\n",
      "  File \"/home/bohumil/FIIT/BP/BP/Zdroje_kod/dist_env/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 541, in __call__\r\n",
      "    result = self.forward(*input, **kwargs)\r\n",
      "  File \"/home/bohumil/FIIT/BP/BP/Zdroje_kod/dist_env/lib/python3.7/site-packages/torchvision/models/resnet.py\", line 199, in forward\r\n",
      "    x = self.maxpool(x)\r\n",
      "  File \"/home/bohumil/FIIT/BP/BP/Zdroje_kod/dist_env/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 541, in __call__\r\n",
      "    result = self.forward(*input, **kwargs)\r\n",
      "  File \"/home/bohumil/FIIT/BP/BP/Zdroje_kod/dist_env/lib/python3.7/site-packages/torch/nn/modules/pooling.py\", line 141, in forward\r\n",
      "    self.return_indices)\r\n",
      "  File \"/home/bohumil/FIIT/BP/BP/Zdroje_kod/dist_env/lib/python3.7/site-packages/torch/_jit_internal.py\", line 138, in fn\r\n",
      "    return if_false(*args, **kwargs)\r\n",
      "  File \"/home/bohumil/FIIT/BP/BP/Zdroje_kod/dist_env/lib/python3.7/site-packages/torch/nn/functional.py\", line 488, in _max_pool2d\r\n",
      "    input, kernel_size, stride, padding, dilation, ceil_mode)\r\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 150.00 MiB (GPU 0; 1.96 GiB total capacity; 988.02 MiB already allocated; 50.88 MiB free; 11.98 MiB cached)\r\n"
     ]
    }
   ],
   "source": [
    "! python /home/bohumil/FIIT/BP/BP/Zdroje_kod/distiller/examples/classifier_compression/compress_classifier.py --eval --qe --qe-lapq -a resnet18 --pretrained /home/bohumil/FIIT/BP/BP/Zdroje_kod/imagenet/ --lapq-eval-size 0.01 --lapq-maxiter 2 --qe-config-file /home/bohumil/FIIT/BP/BP/Zdroje_kod/distiller/examples/quantization/post_train_quant/resnet18_imagenet_post_train_lapq.yaml -b 50 -j 2 --lapq-init-mode L3 --lapq-init-method powell --lapq-eval-memoize-dataloader --det --lapq-search-clipping"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# args.qe_config_file = '/home/bohumil/FIIT/BP/BP/Zdroje_kod/quantization_jupyters/resnet18_imagenet_post_train.yaml'\n",
    "args.qe_config_file = '/home/bohumil/FIIT/BP/BP/Zdroje_kod/distiller/examples/quantization/post_train_quant/resnet18_imagenet_post_train_lapq.yaml'\n",
    "# --qe-config-file /home/bohumil/FIIT/BP/BP/Zdroje_kod/distiller/examples/quantization/post_train_quant/resnet18_imagenet_post_train_lapq.yaml --qe-lapq --lapq-search-clipping\n",
    "args.qe_lapq = true\n",
    "args.laps_search_clipping = true\n",
    "args.lapq_maxiter = 2\n",
    "args.lapq_init_mode = 'L1'\n",
    "args.lapq_init_method = 'Powell'\n",
    "args.lapq_eval_size = 0.1\n",
    "args.lapq_eval_memoize_dataloader = true\n",
    "eval_quantized(model, args)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id='4bit-loss'> </a>\n",
    "\n",
    "# 4 bit Loss-Aware quantization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}