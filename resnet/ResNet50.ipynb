{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# ResNet kvantiz√°cia\n",
    "\n",
    "\n",
    "## Obsah\n",
    "*   [Predspracovanie](#predspracovanie)\n",
    "<a href='#predspracovanie'> </a>\n",
    "*   [Statistiky na staticku kvant.](#stats)\n",
    "<a href='#stats'> </a>\n",
    "*   [Base precision](#base)\n",
    "<a href='#base'> </a>\n",
    "*   [8 bit kvantizacia](#8bit)\n",
    "<a href='#8bit'> </a>\n",
    "*   [4 bit kvantizacia](#4bit)\n",
    "<a href='#4bit'> </a>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3.1\n",
      "0.4.2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import distiller\n",
    "from distiller.models import create_model\n",
    "\n",
    "\n",
    "print(torch.__version__)\n",
    "print(torchvision.__version__)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id='predspracovanie'> </a>\n",
    "\n",
    "# Predspracovanie"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "model = create_model(pretrained=True,dataset='imagenet',arch='resnet50') "
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "DataParallel(\n  (module): ResNet(\n    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (relu): ReLU(inplace=True)\n    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (layer1): Sequential(\n      (0): DistillerBottleneck(\n        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (add): EltwiseAdd()\n        (relu3): ReLU(inplace=True)\n      )\n      (1): DistillerBottleneck(\n        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (add): EltwiseAdd()\n        (relu3): ReLU(inplace=True)\n      )\n      (2): DistillerBottleneck(\n        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (add): EltwiseAdd()\n        (relu3): ReLU(inplace=True)\n      )\n    )\n    (layer2): Sequential(\n      (0): DistillerBottleneck(\n        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (add): EltwiseAdd()\n        (relu3): ReLU(inplace=True)\n      )\n      (1): DistillerBottleneck(\n        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (add): EltwiseAdd()\n        (relu3): ReLU(inplace=True)\n      )\n      (2): DistillerBottleneck(\n        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (add): EltwiseAdd()\n        (relu3): ReLU(inplace=True)\n      )\n      (3): DistillerBottleneck(\n        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (add): EltwiseAdd()\n        (relu3): ReLU(inplace=True)\n      )\n    )\n    (layer3): Sequential(\n      (0): DistillerBottleneck(\n        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (add): EltwiseAdd()\n        (relu3): ReLU(inplace=True)\n      )\n      (1): DistillerBottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (add): EltwiseAdd()\n        (relu3): ReLU(inplace=True)\n      )\n      (2): DistillerBottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (add): EltwiseAdd()\n        (relu3): ReLU(inplace=True)\n      )\n      (3): DistillerBottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (add): EltwiseAdd()\n        (relu3): ReLU(inplace=True)\n      )\n      (4): DistillerBottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (add): EltwiseAdd()\n        (relu3): ReLU(inplace=True)\n      )\n      (5): DistillerBottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (add): EltwiseAdd()\n        (relu3): ReLU(inplace=True)\n      )\n    )\n    (layer4): Sequential(\n      (0): DistillerBottleneck(\n        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (downsample): Sequential(\n          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n        (add): EltwiseAdd()\n        (relu3): ReLU(inplace=True)\n      )\n      (1): DistillerBottleneck(\n        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (add): EltwiseAdd()\n        (relu3): ReLU(inplace=True)\n      )\n      (2): DistillerBottleneck(\n        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu1): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (relu2): ReLU(inplace=True)\n        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (add): EltwiseAdd()\n        (relu3): ReLU(inplace=True)\n      )\n    )\n    (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n    (fc): Linear(in_features=2048, out_features=1000, bias=True)\n  )\n)"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from torchvision import transforms\n",
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                 std=[0.229, 0.224, 0.225])\n",
    "preprocessing = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    normalize,\n",
    "])\n",
    "\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "batch_size = 20\n",
    "num_workers = 1\n",
    "dataset = torchvision.datasets.ImageFolder('/home/bohumil/FIIT/BP/BP/Zdroje_kod/imagenet/val'\n",
    "                                           ,preprocessing)\n",
    "\n",
    "small, big = torch.utils.data.random_split(dataset,[7000, len(dataset)-7000])\n",
    " \n",
    "dataloader = torch.utils.data.DataLoader(small,batch_size=batch_size,\n",
    "                                         num_workers=num_workers,shuffle=True)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from resnet_output import resnet_output\n",
    "\n",
    "def target_labels(dataset,target):\n",
    "    list = target.tolist()\n",
    "    for i in range(len(list)):\n",
    "        list[i] = dataset.classes[list[i]]\n",
    "        list[i] = resnet_output[list[i]]\n",
    "    return torch.LongTensor(list)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# z <distiller_root>/jupyter/post_train_quant_convert_pytorch.ipynb\n",
    "import torchnet as tnt\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def eval_model(data_loader, model, device='cpu', print_freq=10):\n",
    "    # print('Evaluation model ', model.arch)\n",
    "    \n",
    "    criterion = torch.nn.CrossEntropyLoss().to(device)\n",
    "    \n",
    "    loss = tnt.meter.AverageValueMeter()\n",
    "    classerr = tnt.meter.ClassErrorMeter(accuracy=True, topk=(1, 5))\n",
    "    # apmeter = tnt.meter.APMeter()\n",
    "\n",
    "    total_samples = len(data_loader.sampler)\n",
    "    batch_size = data_loader.batch_size\n",
    "    total_steps = math.ceil(total_samples / batch_size)\n",
    "    print('{0} samples ({1} per mini-batch)'.format(total_samples, batch_size))\n",
    "\n",
    "    # Switch to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    for step, (inputs, target) in enumerate(data_loader):\n",
    "        with torch.no_grad():\n",
    "            inputs, target = inputs.to(device), target.to(device)\n",
    "            # compute output from model\n",
    "            output = model(inputs)\n",
    "            target = target_labels(dataset,target).to(device)\n",
    "            # compute loss and measure accuracy\n",
    "            loss.add(criterion(output, target).item())\n",
    "            classerr.add(output.data, target)\n",
    "\n",
    "            if (step + 1) % print_freq == 0:\n",
    "                print('[{:3d}/{:3d}] Top1: {:.3f}  Top5: {:.3f}  Loss: {:.3f}'.format(\n",
    "                      step + 1, total_steps, classerr.value(1), classerr.value(5), loss.mean), flush=True)\n",
    "    print('----------')\n",
    "    print('Overall ==> Top1: {:.3f}  Top5: {:.3f}  Loss: {:.3f}  PPL: {:.3f}'.format(\n",
    "        classerr.value(1), classerr.value(5), loss.mean, np.exp(loss.mean)), flush=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "import logging\n",
    "def config_notebooks_logger():\n",
    "    logging.config.fileConfig('logging.conf')\n",
    "    msglogger = logging.getLogger()\n",
    "    msglogger.info('Logging configured successfully')\n",
    "    return msglogger"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Logging configured successfully\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import distiller\n",
    "\n",
    "msglogger = config_notebooks_logger()\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "distiller.quantization.add_post_train_quant_args(parser)\n",
    "args = parser.parse_args(args= [])\n",
    "# args.qe_config_file = '/home/bohumil/FIIT/BP/BP/Zdroje_kod/quantization_jupyters/resnet50_imagenet_post_train.yaml'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id='stats'> </a>\n",
    "\n",
    "# Correct way of getting statistics\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "cpu_model = distiller.make_non_parallel_copy(model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "text": [
      "Generating quantization calibration stats based on 0.2 users\n"
     ],
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": [
      "3887 samples (20 per mini-batch)\n",
      "[ 30/195] Top1: 86.333  Top5: 97.500  Loss: 0.516\n",
      "[ 60/195] Top1: 87.083  Top5: 97.750  Loss: 0.478\n",
      "[ 90/195] Top1: 87.111  Top5: 97.722  Loss: 0.463\n",
      "[120/195] Top1: 87.000  Top5: 97.667  Loss: 0.465\n",
      "[150/195] Top1: 87.367  Top5: 97.533  Loss: 0.466\n",
      "[180/195] Top1: 87.306  Top5: 97.417  Loss: 0.470\n",
      "----------\n",
      "Overall ==> Top1: 87.471  Top5: 97.453  Loss: 0.464  PPL: 1.591\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "from distiller.data_loggers import collect_quant_stats, QuantCalibrationStatsCollector, collector_context\n",
    "\n",
    "\n",
    "args.qe_calibration = 0.2\n",
    "if args.qe_calibration:\n",
    "    \n",
    "    cpu_model = distiller.make_non_parallel_copy(model).cpu()\n",
    "    \n",
    "    distiller.utils.assign_layer_fq_names(cpu_model)\n",
    "    msglogger.info(\"Generating quantization calibration stats based on {0} users\".format(args.qe_calibration))\n",
    "    collector = distiller.data_loggers.QuantCalibrationStatsCollector(cpu_model)\n",
    "    \n",
    "    part = int(len(dataset)*args.qe_calibration)\n",
    "    batch_size = 20\n",
    "    num_workers = 1\n",
    "    small, big = torch.utils.data.random_split(dataset,[part, len(dataset)-part])\n",
    "    stat_loader = torch.utils.data.DataLoader(small,batch_size=batch_size,\n",
    "                                         num_workers=num_workers,shuffle=True)\n",
    "    \n",
    "    with collector_context(collector):\n",
    "        eval_model(stat_loader,cpu_model,'cpu',print_freq=30)\n",
    "        # Here call your model evaluation function, making sure to execute only\n",
    "        # the portion of the dataset specified by the qe_calibration argument\n",
    "    yaml_path = './resnet50_quantization_stats.yaml'\n",
    "    collector.save(yaml_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a href='#base'> </a>\n",
    "\n",
    "# Base precision"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000 samples (20 per mini-batch)\n",
      "[ 40/350] Top1: 87.250  Top5: 98.125  Loss: 0.471\n",
      "[ 80/350] Top1: 86.188  Top5: 97.562  Loss: 0.501\n",
      "[120/350] Top1: 86.458  Top5: 97.375  Loss: 0.490\n",
      "[160/350] Top1: 87.344  Top5: 97.406  Loss: 0.472\n",
      "[200/350] Top1: 87.000  Top5: 97.400  Loss: 0.479\n",
      "[240/350] Top1: 87.146  Top5: 97.354  Loss: 0.470\n",
      "[280/350] Top1: 86.929  Top5: 97.393  Loss: 0.475\n",
      "[320/350] Top1: 86.922  Top5: 97.531  Loss: 0.475\n",
      "----------\n",
      "Overall ==> Top1: 87.043  Top5: 97.557  Loss: 0.470  PPL: 1.600\n",
      "CPU times: user 5min 32s, sys: 1.16 s, total: 5min 33s\n",
      "Wall time: 5min 31s\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    %time eval_model(dataloader,model,'cuda', print_freq=40)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "args.quantize_eval = True"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "def eval_quantized(model, args):\n",
    "    if args.quantize_eval:\n",
    "        quantizer = distiller.quantization.PostTrainLinearQuantizer.from_args(deepcopy(model), args)\n",
    "        # dummy = distiller.get_dummy_input(model.input_shape)\n",
    "        dummy = distiller.get_dummy_input(input_shape=model.input_shape)\n",
    "        quantizer.prepare_model(dummy)\n",
    "        eval_model(dataloader, quantizer.model, 'cuda', print_freq=30)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id='8bit'> </a>\n",
    "\n",
    "# 8 bit quantization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading configuration from: /home/bohumil/FIIT/BP/BP/Zdroje_kod/quantization_jupyters/resnet/resnet50/resnet50_imagenet_post_train.yaml\n",
      "Found component of class PostTrainLinearQuantizer: Name: post_train_quantizer ; Section: quantizers\n",
      "Loading activation stats from: /home/bohumil/FIIT/BP/BP/Zdroje_kod/quantization_jupyters/resnet/resnet50_quantization_stats.yaml\n",
      "Preparing model for quantization using PostTrainLinearQuantizer\n",
      "Applying batch-norm folding ahead of post-training quantization\n",
      "Propagating output statistics from BN modules to folded modules\n",
      "Optimizing output statistics for modules followed by ReLU/Tanh/Sigmoid\n",
      "Updated stats saved to ./quant_stats_after_prepare_model.yaml\n",
      "Per-layer quantization parameters saved to ./layer_quant_params.yaml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000 samples (20 per mini-batch)\n",
      "[ 30/350] Top1: 86.167  Top5: 97.500  Loss: 0.459\n",
      "[ 60/350] Top1: 86.000  Top5: 97.417  Loss: 0.465\n",
      "[ 90/350] Top1: 86.389  Top5: 97.389  Loss: 0.475\n",
      "[120/350] Top1: 86.625  Top5: 97.625  Loss: 0.466\n",
      "[150/350] Top1: 86.633  Top5: 97.667  Loss: 0.467\n",
      "[180/350] Top1: 86.639  Top5: 97.583  Loss: 0.473\n",
      "[210/350] Top1: 86.595  Top5: 97.429  Loss: 0.478\n",
      "[240/350] Top1: 86.646  Top5: 97.562  Loss: 0.475\n",
      "[270/350] Top1: 86.870  Top5: 97.611  Loss: 0.470\n",
      "[300/350] Top1: 86.833  Top5: 97.533  Loss: 0.471\n",
      "[330/350] Top1: 86.864  Top5: 97.545  Loss: 0.472\n",
      "----------\n",
      "Overall ==> Top1: 86.829  Top5: 97.500  Loss: 0.473  PPL: 1.604\n"
     ]
    }
   ],
   "source": [
    "args.qe_config_file = '/home/bohumil/FIIT/BP/BP/Zdroje_kod/quantization_jupyters/resnet/resnet50/resnet50_imagenet_post_train.yaml'\n",
    "eval_quantized(model, args)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<a id='4bit'> </a>\n",
    "\n",
    "# 4 bit quantization\n",
    "\n",
    "## Run 1\n",
    "```python\n",
    "class: PostTrainLinearQuantizer\n",
    "bits_activations: 4\n",
    "bits_parameters: 4\n",
    "bits_accum: 16\n",
    "mode: ASYMMETRIC_UNSIGNED\n",
    "per_channel_wts: True\n",
    "clip_acts: AVG\n",
    "\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading configuration from: /home/bohumil/FIIT/BP/BP/Zdroje_kod/quantization_jupyters/resnet/resnet50/resnet50_imagenet_post_train_4bit.yaml\n",
      "Found component of class PostTrainLinearQuantizer: Name: post_train_quantizer ; Section: quantizers\n",
      "Loading activation stats from: /home/bohumil/FIIT/BP/BP/Zdroje_kod/quantization_jupyters/resnet/resnet50_quantization_stats.yaml\n",
      "Preparing model for quantization using PostTrainLinearQuantizer\n",
      "Applying batch-norm folding ahead of post-training quantization\n",
      "Propagating output statistics from BN modules to folded modules\n",
      "Optimizing output statistics for modules followed by ReLU/Tanh/Sigmoid\n",
      "Updated stats saved to ./quant_stats_after_prepare_model.yaml\n",
      "Per-layer quantization parameters saved to ./layer_quant_params.yaml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000 samples (20 per mini-batch)\n",
      "[ 30/350] Top1: 20.667  Top5: 37.167  Loss: 5.362\n",
      "[ 60/350] Top1: 18.667  Top5: 36.500  Loss: 5.449\n",
      "[ 90/350] Top1: 19.167  Top5: 37.556  Loss: 5.379\n",
      "[120/350] Top1: 19.958  Top5: 37.792  Loss: 5.331\n",
      "[150/350] Top1: 19.467  Top5: 37.200  Loss: 5.348\n",
      "[180/350] Top1: 18.944  Top5: 36.750  Loss: 5.397\n",
      "[210/350] Top1: 19.000  Top5: 37.143  Loss: 5.360\n",
      "[240/350] Top1: 19.063  Top5: 37.146  Loss: 5.353\n",
      "[270/350] Top1: 19.593  Top5: 37.481  Loss: 5.322\n",
      "[300/350] Top1: 19.583  Top5: 37.433  Loss: 5.330\n",
      "[330/350] Top1: 19.576  Top5: 37.167  Loss: 5.345\n",
      "----------\n",
      "Overall ==> Top1: 19.657  Top5: 37.357  Loss: 5.334  PPL: 207.271\n"
     ]
    }
   ],
   "source": [
    "args.qe_config_file = '/home/bohumil/FIIT/BP/BP/Zdroje_kod/quantization_jupyters/resnet/resnet50/resnet50_imagenet_post_train_4bit.yaml'\n",
    "eval_quantized(model, args);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Uprava parametrov\n",
    "## Run 2\n",
    "/home/bohumil/FIIT/BP/BP/Zdroje_kod/quantization_jupyters/resnet/resnet50/resnet50_imagenet_post_train_4bit2.yaml\n",
    "\n",
    "```python\n",
    "quantizers:\n",
    "  post_train_quantizer:\n",
    "    class: PostTrainLinearQuantizer\n",
    "    bits_activations: 4\n",
    "    bits_parameters: 4\n",
    "    bits_accum: 16\n",
    "\n",
    "    mode: ASYMMETRIC_UNSIGNED\n",
    "    \n",
    "    model_activation_stats: /home/bohumil/FIIT/BP/BP/Zdroje_kod/quantization_jupyters/acts_quantization_stats.yaml\n",
    "    per_channel_wts: True\n",
    "    clip_acts: AVG\n",
    "\n",
    "    overrides:\n",
    "      fc:\n",
    "        clip_acts: NONE  # Don't clip activations in last layer before softmax\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading configuration from: /home/bohumil/FIIT/BP/BP/Zdroje_kod/quantization_jupyters/resnet/resnet50/resnet50_imagenet_post_train_4bit2.yaml\n",
      "Found component of class PostTrainLinearQuantizer: Name: post_train_quantizer ; Section: quantizers\n",
      "Loading activation stats from: /home/bohumil/FIIT/BP/BP/Zdroje_kod/quantization_jupyters/resnet/resnet50_quantization_stats.yaml\n",
      "Preparing model for quantization using PostTrainLinearQuantizer\n",
      "Applying batch-norm folding ahead of post-training quantization\n",
      "Propagating output statistics from BN modules to folded modules\n",
      "Optimizing output statistics for modules followed by ReLU/Tanh/Sigmoid\n",
      "Updated stats saved to ./quant_stats_after_prepare_model.yaml\n",
      "Per-layer quantization parameters saved to ./layer_quant_params.yaml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000 samples (20 per mini-batch)\n",
      "[ 30/350] Top1: 18.167  Top5: 34.500  Loss: 6.043\n",
      "[ 60/350] Top1: 17.333  Top5: 34.917  Loss: 6.089\n",
      "[ 90/350] Top1: 17.944  Top5: 35.611  Loss: 5.991\n",
      "[120/350] Top1: 18.250  Top5: 35.542  Loss: 5.928\n",
      "[150/350] Top1: 18.300  Top5: 35.533  Loss: 5.950\n",
      "[180/350] Top1: 18.000  Top5: 35.750  Loss: 5.956\n",
      "[210/350] Top1: 17.857  Top5: 35.833  Loss: 5.933\n",
      "[240/350] Top1: 17.875  Top5: 35.604  Loss: 5.931\n",
      "[270/350] Top1: 17.741  Top5: 35.500  Loss: 5.941\n",
      "[300/350] Top1: 17.900  Top5: 35.750  Loss: 5.935\n",
      "[330/350] Top1: 17.970  Top5: 35.803  Loss: 5.945\n",
      "----------\n",
      "Overall ==> Top1: 17.971  Top5: 35.629  Loss: 5.958  PPL: 386.699\n"
     ]
    }
   ],
   "source": [
    "args.qe_config_file = '/home/bohumil/FIIT/BP/BP/Zdroje_kod/quantization_jupyters/resnet/resnet50/resnet50_imagenet_post_train_4bit2.yaml'\n",
    "eval_quantized(model, args)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "```python\n",
    " overrides:\n",
    "    # First and last layers in 8-bits\n",
    "      conv1:\n",
    "        bits_weights: 8\n",
    "        bits_activations: 8\n",
    "      fc:\n",
    "        bits_weights: 8\n",
    "        bits_activations: 8\n",
    "        clip_acts: NONE  # Don't clip activations in last layer before softmax\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n",
     "is_executing": false
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading configuration from: /home/bohumil/FIIT/BP/BP/Zdroje_kod/quantization_jupyters/resnet/resnet50/resnet50_imagenet_post_train_4bit3.yaml\n",
      "Found component of class PostTrainLinearQuantizer: Name: post_train_quantizer ; Section: quantizers\n",
      "Loading activation stats from: /home/bohumil/FIIT/BP/BP/Zdroje_kod/quantization_jupyters/resnet/resnet50_quantization_stats.yaml\n",
      "Preparing model for quantization using PostTrainLinearQuantizer\n",
      "Applying batch-norm folding ahead of post-training quantization\n",
      "Propagating output statistics from BN modules to folded modules\n",
      "Optimizing output statistics for modules followed by ReLU/Tanh/Sigmoid\n",
      "Updated stats saved to ./quant_stats_after_prepare_model.yaml\n",
      "Per-layer quantization parameters saved to ./layer_quant_params.yaml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000 samples (20 per mini-batch)\n",
      "[ 30/350] Top1: 0.333  Top5: 0.667  Loss: 8.366\n",
      "[ 60/350] Top1: 0.250  Top5: 0.750  Loss: 8.281\n",
      "[ 90/350] Top1: 0.222  Top5: 0.667  Loss: 8.275\n",
      "[120/350] Top1: 0.167  Top5: 0.625  Loss: 8.255\n",
      "[150/350] Top1: 0.133  Top5: 0.633  Loss: 8.271\n",
      "[180/350] Top1: 0.139  Top5: 0.583  Loss: 8.256\n",
      "[210/350] Top1: 0.143  Top5: 0.619  Loss: 8.239\n",
      "[240/350] Top1: 0.125  Top5: 0.583  Loss: 8.239\n",
      "[270/350] Top1: 0.111  Top5: 0.667  Loss: 8.234\n",
      "[300/350] Top1: 0.100  Top5: 0.633  Loss: 8.233\n",
      "[330/350] Top1: 0.121  Top5: 0.636  Loss: 8.246\n",
      "----------\n",
      "Overall ==> Top1: 0.114  Top5: 0.614  Loss: 8.252  PPL: 3834.065\n"
     ]
    }
   ],
   "source": [
    "args.qe_config_file = '/home/bohumil/FIIT/BP/BP/Zdroje_kod/quantization_jupyters/resnet/resnet50/resnet50_imagenet_post_train_4bit3.yaml'\n",
    "eval_quantized(model, args)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Mixed model\n",
    "\n",
    "```python\n",
    "quantizers:\n",
    "  post_train_quantizer:\n",
    "    class: PostTrainLinearQuantizer\n",
    "    bits_activations: 8\n",
    "    bits_parameters: 4\n",
    "    bits_accum: 16\n",
    "\n",
    "    mode: ASYMMETRIC_UNSIGNED\n",
    "   \n",
    "    model_activation_stats: /home/bohumil/FIIT/BP/BP/Zdroje_kod/quantization_jupyters/acts_quantization_stats.yaml\n",
    "    per_channel_wts: True\n",
    "    clip_acts: AVG\n",
    "\n",
    "    overrides:\n",
    "    # First and last layers in 8-bits\n",
    "      conv1:\n",
    "        bits_weights: 8\n",
    "        bits_activations: 8\n",
    "      fc:\n",
    "        bits_weights: 8\n",
    "        bits_activations: 8\n",
    "        clip_acts: NONE  # Don't clip activations in last layer before softmax\n",
    "```"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reading configuration from: /home/bohumil/FIIT/BP/BP/Zdroje_kod/quantization_jupyters/resnet/resnet50/resnet50_imagenet_post_train_4bit4.yaml\n",
      "Found component of class PostTrainLinearQuantizer: Name: post_train_quantizer ; Section: quantizers\n",
      "Loading activation stats from: /home/bohumil/FIIT/BP/BP/Zdroje_kod/quantization_jupyters/resnet/resnet50_quantization_stats.yaml\n",
      "Preparing model for quantization using PostTrainLinearQuantizer\n",
      "Applying batch-norm folding ahead of post-training quantization\n",
      "Propagating output statistics from BN modules to folded modules\n",
      "Optimizing output statistics for modules followed by ReLU/Tanh/Sigmoid\n",
      "Updated stats saved to ./quant_stats_after_prepare_model.yaml\n",
      "Per-layer quantization parameters saved to ./layer_quant_params.yaml\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7000 samples (20 per mini-batch)\n",
      "[ 30/350] Top1: 0.000  Top5: 0.667  Loss: 8.109\n",
      "[ 60/350] Top1: 0.000  Top5: 0.583  Loss: 8.031\n",
      "[ 90/350] Top1: 0.000  Top5: 0.667  Loss: 8.069\n",
      "[120/350] Top1: 0.000  Top5: 0.625  Loss: 8.046\n",
      "[150/350] Top1: 0.000  Top5: 0.600  Loss: 8.051\n",
      "[180/350] Top1: 0.000  Top5: 0.611  Loss: 8.055\n",
      "[210/350] Top1: 0.024  Top5: 0.595  Loss: 8.086\n",
      "[240/350] Top1: 0.021  Top5: 0.521  Loss: 8.086\n",
      "[270/350] Top1: 0.037  Top5: 0.500  Loss: 8.095\n",
      "[300/350] Top1: 0.033  Top5: 0.533  Loss: 8.090\n",
      "[330/350] Top1: 0.030  Top5: 0.561  Loss: 8.086\n",
      "----------\n",
      "Overall ==> Top1: 0.029  Top5: 0.557  Loss: 8.093  PPL: 3270.066\n"
     ]
    }
   ],
   "source": [
    "args.qe_config_file = '/home/bohumil/FIIT/BP/BP/Zdroje_kod/quantization_jupyters/resnet/resnet50/resnet50_imagenet_post_train_4bit4.yaml'\n",
    "eval_quantized(model, args)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": false
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}